#-------------------------------------------------------------
#Matrix Factorization

library(dslabs)
library(tidyverse)
library(dplyr)
data("movielens")

head(movielens)

train_small <- movielens %>% 
  group_by(movieId) %>%
  filter(n() >= 50 | movieId == 3252) %>% ungroup() %>% #3252 is Scent of a Woman used in example
  group_by(userId) %>%
  filter(n() >= 50) %>% ungroup()

y <- train_small %>% 
  select(userId, movieId, rating) %>%
  spread(movieId, rating) %>%
  as.matrix()

rownames(y)<- y[,1]
y <- y[,-1]
colnames(y) <- with(movie_titles, title[match(colnames(y), movieId)])

y <- sweep(y, 1, rowMeans(y, na.rm=TRUE))
y <- sweep(y, 2, colMeans(y, na.rm=TRUE))

m_1 <- "Godfather, The"
m_2 <- "Godfather: Part II, The"
qplot(y[ ,m_1], y[,m_2], xlab = m_1, ylab = m_2)

m_1 <- "Godfather, The"
m_3 <- "Goodfellas"
qplot(y[ ,m_1], y[,m_3], xlab = m_1, ylab = m_3)

m_4 <- "You've Got Mail" 
m_5 <- "Sleepless in Seattle" 
qplot(y[ ,m_4], y[,m_5], xlab = m_4, ylab = m_5)

cor(y[, c(m_1, m_2, m_3, m_4, m_5)], use="pairwise.complete") %>% 
  knitr::kable()

set.seed(1)
options(digits = 2)
Q <- matrix(c(1 , 1, 1, -1, -1), ncol=1)
rownames(Q) <- c(m_1, m_2, m_3, m_4, m_5)
P <- matrix(rep(c(2,0,-2), c(3,5,4)), ncol=1)
rownames(P) <- 1:nrow(P)

X <- jitter(P%*%t(Q))
X %>% knitr::kable(align = "c")

cor(X)

t(Q) %>% knitr::kable(aling="c")

P

set.seed(1)
options(digits = 2)
m_6 <- "Scent of a Woman"
Q <- cbind(c(1 , 1, 1, -1, -1, -1), 
           c(1 , 1, -1, -1, -1, 1))
rownames(Q) <- c(m_1, m_2, m_3, m_4, m_5, m_6)
P <- cbind(rep(c(2,0,-2), c(3,5,4)), 
           c(-1,1,1,0,0,1,1,1,0,-1,-1,-1))/2
rownames(P) <- 1:nrow(X)

X <- jitter(P%*%t(Q), factor=1)
X %>% knitr::kable(align = "c")

cor(X)

t(Q) %>% knitr::kable(aling="c")

P

six_movies <- c(m_1, m_2, m_3, m_4, m_5, m_6)
tmp <- y[,six_movies]
cor(tmp, use="pairwise.complete")

#-------------------------------------------------------------
#SVD and PCA

y[is.na(y)] <- 0
y <- sweep(y, 1, rowMeans(y))
pca <- prcomp(y)

dim(pca$rotation)

dim(pca$x)

plot(pca$sdev)

var_explained <- cumsum(pca$sdev^2/sum(pca$sdev^2))
plot(var_explained)

library(ggrepel)
pcs <- data.frame(pca$rotation, name = colnames(y))
pcs %>%  ggplot(aes(PC1, PC2)) + geom_point() + 
  geom_text_repel(aes(PC1, PC2, label=name),
                  data = filter(pcs, 
                                PC1 < -0.1 | PC1 > 0.1 | PC2 < -0.075 | PC2 > 0.1))

pcs %>% select(name, PC1) %>% arrange(PC1) %>% slice(1:10)

pcs %>% select(name, PC1) %>% arrange(desc(PC1)) %>% slice(1:10)

pcs %>% select(name, PC2) %>% arrange(PC2) %>% slice(1:10)

pcs %>% select(name, PC2) %>% arrange(desc(PC2)) %>% slice(1:10)

#-------------------------------------------------------------
#Comprehension Check: Matrix Factorization

#In this exercise set, we will be covering a topic useful for understanding matrix
#factorization: the singular value decomposition (SVD).

#SVD is a mathematical result that is widely used in machine learning, both in
#practice and to understand the mathematical properties of some algorithms.
#This is a rather advanced topic and to complete this exercise set you will have
#to be familiar with linear algebra concepts such as matrix multiplication,
#orthogonal matrices, and diagonal matrices.

#The SVD tells us that we can decompose an ğ‘Ã—ğ‘ matrix ğ‘Œ with ğ‘<ğ‘ as 
#ğ‘Œ=ğ‘ˆğ·ğ‘‰âŠ¤
#with ğ‘ˆ and ğ‘‰ orthogonal of dimensions ğ‘Ã—ğ‘ and ğ‘Ã—ğ‘ respectively and ğ· a ğ‘Ã—ğ‘
#diagonal matrix with the values of the diagonal decreasing: 
#  ğ‘‘1,1â‰¥ğ‘‘2,2â‰¥â€¦ğ‘‘ğ‘,ğ‘

#In this exercise, we will see one of the ways that this decomposition can be useful.
#To do this, we will construct a dataset that represents grade scores for 100 students
#in 24 different subjects. The overall average has been removed so this data represents
#the percentage point each student received above or below the average test score.
#So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents
#a low grade (F). You can simulate the data like this:

set.seed(1987, sample.kind="Rounding")
n <- 100
k <- 8
Sigma <- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) 
m <- MASS::mvrnorm(n, rep(0, 3), Sigma)
m <- m[order(rowMeans(m), decreasing = TRUE),]
y <- m %x% matrix(rep(1, k), nrow = 1) + matrix(rnorm(matrix(n*k*3)), n, k*3)
colnames(y) <- c(paste(rep("Math",k), 1:k, sep="_"),
                 paste(rep("Science",k), 1:k, sep="_"),
                 paste(rep("Arts",k), 1:k, sep="_"))

#Our goal is to describe the student performances as succinctly as possible.
#For example, we want to know if these test results are all just a random
#independent numbers. Are all students just about as good? Does being good
#in one subject  imply you will be good in another? How does the SVD help
#with all this? We will go step by step to show that with just three
#relatively small pairs of vectors we can explain much of the variability
#in this 100Ã—24 dataset.

#Q1:

#You can visualize the 24 test scores for the 100 students by plotting an image:

my_image <- function(x, zlim = range(x), ...){
  colors = rev(RColorBrewer::brewer.pal(9, "RdBu"))
  cols <- 1:ncol(x)
  rows <- 1:nrow(x)
  image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = "n", yaxt = "n",
        xlab="", ylab="",  col = colors, zlim = zlim, ...)
  abline(h=rows + 0.5, v = cols + 0.5)
  axis(side = 1, cols, colnames(x), las = 2)
}

my_image(y)

#How would you describe the data based on this figure?

#The students that test well are at the top of the image and there
#seem to be three groupings by subject. 

#Q2:

#You can examine the correlation between the test scores directly like this:

my_image(cor(y), zlim = c(-1,1))
range(cor(y))
axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)

#Which of the following best describes what you see?

#There is correlation among all tests, but higher if the tests are in
#science and math and even higher within each subject. 

#Q3:

#Remember that orthogonality means that ğ‘ˆâŠ¤ğ‘ˆ and ğ‘‰âŠ¤ğ‘‰ are equal to
#the identity matrix. This implies that we can also rewrite the
#decomposition as

#ğ‘Œğ‘‰=ğ‘ˆğ· or ğ‘ˆâŠ¤ğ‘Œ=ğ·ğ‘‰âŠ¤

#We can think of ğ‘Œğ‘‰ and ğ‘ˆâŠ¤ğ‘‰ as two transformations of ğ‘Œ that pre
#erve the total variability of ğ‘Œ since ğ‘ˆ and ğ‘‰ are orthogonal.

#Use the function svd to compute the SVD of y. This function will
#return ğ‘ˆ, ğ‘‰, and the diagonal entries of ğ·.

s <- svd(y)
names(s)

#You can check that the SVD works by typing:

y_svd <- s$u %*% diag(s$d) %*% t(s$v)
max(abs(y - y_svd))

#Compute the sum of squares of the columns of ğ‘Œ and store them in ss_y
#Then compute the sum of squares of columns of the transformed ğ‘Œğ‘‰ 
#and store them in ss_yv. Confirm that sum(ss_y) is equal to sum(ss_yv).

sos <- function(column) {
  sum(column^2)
}
ss_y <- apply(y,2, sos)
ss_yv <- apply(y %*% s$v,2, sos)

options(digits=3)

round(sum(ss_y),0)== round(sum(ss_yv))

#What is the value of sum(ss_y) (and also the value of sum(ss_yv))?
sum(ss_y)

#Q4:

#We see that the total sum of squares is preserved. This is because ğ‘‰ is orthogonal
#Now to start understanding how ğ‘Œğ‘‰ is useful, plot ss_y against the column number
# and then do the same for ss_yv.

p <- length(ss_y)
p

ggplot() +
  geom_point(aes(x = 1:p, y = ss_y, color = "ss_y")) + 
  geom_point(aes(x = 1:p, y = ss_yv, color = "ss_yv"))
  
#What do you observe?

#ss_yv is decreasing and close to 0 for the 4th column and beyond. 

#Q5:

#Now notice that we didn't have to compute ss_yv because we already
#have the answer. How? Remember that ğ‘Œğ‘‰=ğ‘ˆğ· and because ğ‘ˆ is orthogonal,
#we know that the sum of squares of the columns of ğ‘ˆğ· are the diagonal
#entries of ğ· squared. Confirm this by plotting the square root of ss_yv
#versus the diagonal entries of ğ·.

ggplot(aes(x = sqrt(ss_yv), y = s$d)) + geom_point()

#Which of these plots is correct?

#The diagonal one

#Q6:

#So from the above we know that the sum of squares of the columns of ğ‘Œ
#(the total sum of squares) adds up to the sum of s$d^2 and that the 
#transformation ğ‘Œğ‘‰ gives us columns with sums of squares equal to s$d^2
#Now compute the percent of the total variability that is explained by
#just the first three columns of ğ‘Œğ‘‰.

library(tidyverse)
library(dslabs)
library(matrixStats)

yv_col_variability <- ss_yv
total_variability <- sum(yv_col_variability)
proportion <- yv_col_variability/total_variability

ggplot() +
  geom_point(aes(x = 1:p, y = proportion, color = "column variability"))


#What proportion of the total variability is explained by the first three
#columns of ğ‘Œğ‘‰?
sum(proportion[1:3])

#Q7:

#Before we continue, let's show a useful computational trick to avoid creating
#the matrix diag(s$d). To motivate this, we note that if we write ğ‘ˆ out in it
#columns [ğ‘ˆ1,ğ‘ˆ2,â€¦,ğ‘ˆğ‘] then ğ‘ˆğ· is equal to

#ğ‘ˆğ·=[ğ‘ˆ1ğ‘‘1,1,ğ‘ˆ2ğ‘‘2,2,â€¦,ğ‘ˆğ‘ğ‘‘ğ‘,ğ‘]

#Use the sweep function to compute ğ‘ˆğ· without constructing diag(s$d) or using
#matrix multiplication.

ud <- s$u %*% diag(s$d)

ud_sweep <- sweep(s$u, 2, s$d, FUN = "*")

identical(ud, ud_sweep)

#Which code is correct?

identical(s$u %*% diag(s$d), sweep(s$u, 2, s$d, FUN = "*"))

#Q8:

#We know that ğ‘ˆ1ğ‘‘1,1, the first column of ğ‘ˆğ·, has the most variability 
#of all the columns of ğ‘ˆğ·. Earlier we looked at an image of ğ‘Œ using my_image(y),
#in which we saw that the student to student variability is quite large and 
#that students that are good in one subject tend to be good in all. This implies 
#that the average (across all subjects) for each student should explain a lot of 
#the variability. Compute the average score for each student, plot it against
#ğ‘ˆ1ğ‘‘1,1, and describe what you find.

stud_avg <- rowMeans(y)

ggplot() +
  geom_point(aes(x=stud_avg, y=-ud[,1]))

#Or alternatively

ggplot() +
  geom_point(aes(x=stud_avg, y=-s$u[,1] * s$d[1]))

#What do you observe?
  
#Q9:

#We note that the signs in SVD are arbitrary because:
  
#  ğ‘ˆğ·ğ‘‰âŠ¤=(âˆ’ğ‘ˆ)ğ·(âˆ’ğ‘‰)âŠ¤

#With this in mind we see that the first column of ğ‘ˆğ· is almost identical to the
#average score for each student except for the sign.

#This implies that multiplying ğ‘Œ by the first column of ğ‘‰ must be performing a
#similar operation to taking the average. Make an image plot of ğ‘‰ and describe 
#the first column relative to others and how this relates to taking an average.

my_image(s$v)

s$v[,1]

s$v[,1]/sum(s$v[,1])

#How does the first column relate to the others, and how does this relate to 
#taking an average?
  
#The first column is very close to being a constant, which implies that the first column of YV is the sum of the rows of Y multiplied by some constant, and is thus proportional to an average
#. 

#-------------------------------------------------------------
#Comprehension 
#Check: Clustering








