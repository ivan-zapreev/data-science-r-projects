---
title: "MovieLens Recommendation System"
author: "Dr. Ivan S. Zapreev"
date: "`r format(Sys.Date())`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(tidyverse)
library(lubridate)
library(graphics)

MOVIELENS_DATA_FILE_NAME <- "movielens_data.rda"
MOVIELENS_REPORT_FILE_NAME <- "movielens_report.rda"

#Load the files
load(MOVIELENS_DATA_FILE_NAME)
load(MOVIELENS_REPORT_FILE_NAME)

#Create some shortcuts
edx_set_info <- movielens_report$edx_set_info
valid_set_info <- movielens_report$validation_set_info
```

*[According to Wikipedia](https://en.wikipedia.org/wiki/Recommender_system):*

> A recommender system or a recommendation system (sometimes replacing 'system' with a synonym such as platform or engine) is a subclass of information filtering system that seeks to predict the "rating" or "preference" a user would give to an item.

# Introduction
<!--an introduction/overview/executive summary section that describes the dataset and summarizes the goal of the project and key steps that were performed-->

Recommendation systems are widely used in commercial application to provide targeted commercials or suggest online content. In the latter, one can think of *Spotify* suggesting songs or *Youtube* proposing videos. The main goal of a recommendation system is to accurately predict client's preferences, based on available data on the previous user behavior. Note that, such data includes the behavior of all the clients.

The goal of this work is to build a movie recommendation system based on the `r movielens_report$MOVIELENS_DATA_SET_NAME` available from [`r movielens_report$MOVIELENS_DATA_SET_SITE_URL`](`r movielens_report$MOVIELENS_DATA_SET_SITE_URL`). To reach our goal we will use supervised machine learning techniques studied within the *"PH125.8x Data Science: Machine Learning"* course (a part of the broader HarvardX *Data Science Professional* certification program). The recommendation system will be therefore based on (linear) statistical models trained on the subset of the `r movielens_report$MOVIELENS_DATA_SET_NAME`.

The remainder of this section first present the data set, then defines the goal of this project more concretely, and finally identifies the main steps to be taken to reach the goal.

## Dataset overview
<!--This sub-section stores the references to the original data set and specified the sub-set thereof that is going to be worked with-->

As stated in the [README](http://files.grouplens.org/datasets/movielens/ml-10m-README.html) of the original `r movielens_report$MOVIELENS_DATA_SET_NAME`, it contains `10000054` ratings and `95580` tags (not used in this study) applied to `10681` movies by `71567` users of the [MovieLens](http://www.movielens.org/) service. 

All ratings are contained in the file `ml-10M100K/ratings.dat`. Each line of this file represents one rating specified by the rating value (`rating`), the movie identifier (`movieId`), the identifier of the user (`userId`) and the submission timestamp (`timestamp`). Ratings are made on a `5`-star scale, with half-star increments. Timestamps represent seconds since midnight *Coordinated Universal Time (UTC) of January 1, 1970*. 

Movie information is contained in the file `ml-10M100K/movies.dat`. Each line of this file represents one movie specified by its identifier (`movieId`), movie title (`title`) and the corresponding genres (`genres`). The movie titles are entered manually, so errors and inconsistencies may exist. Genres are pipe-separated lists, of the individual genres from the following set:

  `("Action", "Adventure", "Animation", "Children's", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical", "Mystery", "Romance", "Sci-Fi", "Thriller", "War", "Western")`.
  
Note that, the MovieLens data set is not used "as is" but is pre-processed. The pre-processing steps will be described and explained in the *"Data preparation"* section of this document.

## Project goal
The goal of the project is to build and train a statistical model predicting movie rating based on, at least, the user and the movie. The model is to be trained on the training set (`edx`) and is to be evaluation on a validation set (`validation`).[^1] 

The evaluation of model will be done using the residual mean squared error (RMSE) which, similarly to a standard deviation, can be interpreted as: *the typical error we make when predicting a movie rating*. In other words, e.g, if this number is larger than `1`, it means our typical error is larger than one star.

**Definition** *(RMSE)*

Let $y_{u,m}$ be the rating for movie $m$ by user $u$ and $\hat{y}_{u,m}$ be our prediction for that rating, then: 

\[
  RMSE=\sqrt{\sum_{m=1}^{N}(y_{u,m}-\hat{y}_{u,m})^2}
\]

The definition above trivially translates into the following `R` function:

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

The ultimate goal of the project is to create a statistical model, solely based on the training set, that on the `validation` set will be able to predict the movie rating with $RMSE <= 0.8649$. The way we build such a model will be explained in the *"Modeling approach"* section of this document.

[^1]: These sets are defined in the *"Data preparation"* section.

## Execution plan

Let us now briefly outline the main steps to be performed to reach the previously formalized project goal:

1. **Prepare the data** -- see the *"Data preparation"* section:
    + Select the relevant data; split into training and validation sets; and etc.
2. **Analyze the dataset** -- see the *"Dataset analysis"* section:
    + Perform data exploration and visualization; summarize insights on the data.
3. **Describe the modeling approach** -- see the *"Modeling approach"* section:
    + Consider the insights of the data analysis; suggest the way for building the prediction model.
4. **Present modeling results** -- see the  *"Results"* section:
    + Train the model on the test set; analyze the training results; evaluate on the validation set.
5. **Provide concluding remarks** -- see the *"Conclusions"* section:
    + Summarize the results; mention any approach limitations; outline possible future improvements.

Please note that, the remainder of the document is structured according to the execution plan listed above.

# Data preparation
<!--This section explains how the original MovieLens data set is pre-processed, cleaned and split to get the training and validation sets for training and validation of the statistical model(s) to be build-->

In this project, we use a pre-processed subset of the original dataset. Let us now explain how that subset obtained and pre-processed. Here we only use the data from the `ratings.dat` and `movies.dat` files from the `r movielens_report$MOVIELENS_DATA_SET_NAME`. To facilitate supervised learning, the data is pre-processed and split into a training (`edx`) and testing (`validation`) sets. The former will be used for training statistical model(s) and the latter will be used for the model(s) validation.

We pre-process and split the data in the following way:

1. The `ratings.dat` and `movies.dat` are loaded into `ratings` and `movies` data frames
    1. `ratings` -- with columns named `r c("userId", "movieId", "rating", "timestamp")`
    2. `movies` -- columns named `r c("movieId", "title", "genres")`
2. The ratings are coupled with the movies information by the `movieId` into a new `movielens` data frame
3. The `movielens` data frame is randomly split into two parts:
    2. `edx` -- a set storing the `r (1-movielens_report$VALIDATION_TO.EDX_SET_RATIO)*100`% of the observations from the `movielens`, to be used for training
    1. `validation` -- a set storing the `r movielens_report$VALIDATION_TO.EDX_SET_RATIO*100`% of the observations from the `movielens`, to be used for validation
4. The `validation` set is filtered to only contain movies and users present in `edx`
5. The observations filtered out in the previous step are added back to the `edx` set

For more details, see the `create_movielens_sets` function located in the `movielens_project.R` script. As a result of pre-processing we have two sets with the following metrics, the training set `edx`:

```{r, echo=F}
as_tibble(edx_set_info)
```

and the validation set `validation`:

```{r, echo=F}
as_tibble(valid_set_info)
```

```{r, echo=F}
percent_obs_edx <- round(edx_set_info$num_observations/(edx_set_info$num_observations+valid_set_info$num_observations)*100)
```
As one can see the `edx` set stores `r percent_obs_edx`% of the total number of observations (`num_observations`) and the `validation` set contains `r 100-percent_obs_edx`%. The difference in the number of distinct movies (`num_movies`) and the number of distinct users (`num_users`) is explained by the observations being moved back from `validation` to `edx` in the last data preprocessing step, described above.

Given the training and validation set preparation procedure above we assume that the resulting sets follow the same probability distribution of the conditional probability of interest, see the section on the *"Modeling approach"*.

At last, let us mention that the `edx` and `validation` set analysis revealed no `N/A` values are present and also there was no need detected to do any additional cleaning of the data.

# Dataset analysis
<!--In this section we analyze the training set, by means of data exploration and visualization, in order to get insights on the data and to highlight its features that could be further used in developing the prediction model-->

In this sections we analyze the training (`edx`) subset of the original MovieLens data set only, as we assume that this is the only data available for building the prediction models. The motivation behind is that we want to avoid influencing the model validation results. This is done under a, not-verified, assumption that the the testing set, follows the same probability distribution of the conditional probability of interest as the validation set. In the *"Results"* section we may come back to this assumption in case the accuracy of the developed statistical model(s) on the validation set will turn out to be insufficient.

From the number of distinct user, movies and observations (ratings) in the `edx` set it is clear that not every user has rated every movie as `r edx_set_info$num_users` \* `r edx_set_info$num_movies` < `r edx_set_info$num_observations`. The same can be seen via the next plot for randomly sampled `100` unique users and `100` unique movies:

```{r, echo=F}
#Sample 100 individual movie ids
movie_ids <- unique(movielens_data$edx$movieId)
sample_movie_ids <- sample(movie_ids, 100)

#Plot the sparse matrix
movielens_data$edx %>%
  filter(movieId %in% sample_movie_ids) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 100) %>%
  spread(userId, rating) %>%
  select(sample(ncol(.), 100)) %>% #Here we sample a 100 individual user ids
  as.matrix() %>%
  t(.) %>%
  image(1:100, 1:100, . , xlab="Users", ylab="Movies") %>%
  abline(h=0:100+0.5, v=0:100+0.5, col = "grey") %>%
  title(main = "Movie ratings sparsity")
```

The the task of a recommendation system we are to build can be seen as filling in the missing ratings.
To stress the complexity of the undertaking, below we list a number of different observations that can influence rating predictions, and may need to be taken into account when building the statistical model.

**(O.I):** Some movies are rated more often than the others:

```{r}
movielens_data$edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  labs(x = "Number of ratings", y = "Number of movies") + 
  ggtitle("Movie rating counts") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.II):** Some users are more active in rating movies that the others:

```{r}
movielens_data$edx %>%
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() +
  labs(x = "Number of ratings", y = "Number of users") + 
  ggtitle("User rating activity") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.III):** Movies differ from each other in given ratings, lets consider `10` movies with at least `100` ratings each:

```{r}
#Sample 10 individual movie ids for the movies with more than 100 ratings
movie_ids <- movielens_data$edx %>%
  group_by(movieId) %>% summarise(cnt = n()) %>% 
  filter(cnt > 100) %>% pull(movieId) %>% unique(.)
set.seed(1)
sample_movie_ids <- sample(movie_ids, 10)

#Use the box plot to show variations in ratings between different movies
movielens_data$edx %>% 
  filter(movieId %in% sample_movie_ids) %>% 
  ggplot(aes(title, rating, group=movieId)) +
  geom_boxplot() +
  scale_x_discrete(labels = paste("movieId ",as.character(sample_movie_ids)))+
  theme(axis.text.x = element_text(angle = 20)) + 
  labs(x = "Selected movies", y="Received ratings") + 
  ggtitle("Movie effects") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.IV):** Users differ from each other in giving ratings, lets consider `10` users with at least `100` ratings each:

```{r}
#Sample 10 individual user ids for the users with more than 100 ratings
user_ids <- movielens_data$edx %>%
  group_by(userId) %>% summarise(cnt = n()) %>% 
  filter(cnt > 100) %>% pull(userId) %>% unique(.)
set.seed(5)
sample_user_ids <- sample(user_ids, 10)

#Use the boxplot to show variations in ratings between different users
movielens_data$edx %>% 
  filter(userId %in% sample_user_ids) %>% 
  mutate(userName = paste("userId ", as.character(userId))) %>%
  group_by(userId) %>%
  ggplot(aes(userName, rating, group=userId)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 20)) +
  labs(x = "Selected users", y= "Given ratings") + 
  ggtitle("User effects") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.V):** The best and the worst (on-average) movies were not rated very often:

```{r, echo=F}
#Per movie, get the average movies score and rating counts
avg_movie_ratings <- movielens_data$edx %>%
  group_by(movieId) %>%
  summarize(avg_rating = mean(rating),
            num_ratings = n()) %>%
  arrange(desc(avg_rating))
```

```{r}
#Get the movies with the top 10 average scores
true_top_10 <- avg_movie_ratings %>%
  arrange(desc(avg_rating)) %>% 
  slice(1:10)
true_top_10

#Get the movies with the bottom 10 average scores
true_bottom_10 <- avg_movie_ratings %>%
  arrange(avg_rating) %>% 
  slice(1:10)
true_bottom_10
```

See also the next plot where the best movies are marked as blue and the worst as red:
```{r}
#Plot the movies
avg_movie_ratings %>%
  ggplot(aes(x = num_ratings, y = avg_rating)) +
  geom_point(aes(color = I("black")), alpha=0.2)+
  geom_point(data = true_top_10, aes(num_ratings, avg_rating), color="blue") +
  geom_point(data = true_bottom_10, aes(num_ratings, avg_rating), color="red") +
  labs(x="Number of movie ratings", y="Average movie rating") +
  ggtitle("Best and worst movie effects") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.VI):** The rating scores depend on the time when the rating is given:

```{r}
#Convert the timestamp into a week (date) and then plot 
#the mean rating for all the movies per week
movielens_data$edx %>%
  mutate(date = as_datetime(timestamp), week = round_date(date, "weekId")) %>%
  group_by(weekId) %>%
  summarize(avg_rating = mean(rating)) %>%
  ggplot(aes(weekId, avg_rating)) +
  geom_line() +
  geom_smooth(method = 'loess', method.args=list(degree=2)) +
  labs(x="Week", y="Average rating") +
  ggtitle("Global timing effects") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.VII):** The ratings show dependency from the movie's popularity is rated:

As we already see from the plot above, there is more variability for the movies with fewer ratings. Let us now consider the same plot with the smoothened average movie rating trend line:
```{r}
#Plot the movies
avg_movie_ratings %>%
  ggplot(aes(x = num_ratings, y = avg_rating)) +
  geom_point(aes(color = I("black")), alpha=0.2) +
  geom_smooth(method = 'loess', method.args=list(degree=2)) +
  labs(x="Number of movie ratings", y="Average movie rating") +
  ggtitle("Movie popularity effects") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
We can clearly see that the movies that are rated more often (we call them "popular" movies) are rated higher on-average.

In addition we could consider the effects caused by the movie titles and genres. One could think of analyzing the influence of:

* Keywords and phrases in the movie titles
* Connection between sequels, prequels, and spin-offs
* Movie genre and genre combinations

Although possible and interesting, due to the lack of time, the latter will not be considered in this study.

# Modeling approach
<!--This section describes the modeling approach as based on the insights obtained in the previous section-->

Consider the columns of the available MovieLens data:
```{r, echo=F}
names(movielens_data$edx)
```
Then the conditional probability distribution that is to be estimates can be formalized as:
\[
Pr\left(R \; \vert \; M = m, \;U = u, \;T = t, \;G = g, \;L = l\right)
\]
Here the movie-related random variables are:

* $R$ -- the rating to be given; 
* $U$ -- the user id; 
* $M$ -- the movie id; 
* $T$ -- the ratings timestamp; 
* $G$ -- the movie genres; 
* $L$ -- the movie title; 

So here we could use $5$ predictors do provide the estimates of $R$.

As discussion in the end of the *"Dataset analysis"* section, we shall NOT consider the effects caused by the genres ($G$) and movie titles ($T$). This leaves us with three predictors $U$, $M$ and $T$ and the conditional probability distribution to be estimated:
\[
Pr\left(R \; \vert \; M = m, \;U = u, \;T = t\right)
\]

Before we proceed, let us note that the dimension reduction techniques, e.g. Principal Component Analysis (PCA), are not very useful here as the number of predictors is small, so they will not be used. Moreover, due to the large amount of data in the dataset, the standard available statistical models in $R$, such as the Linear Model (`lm`) or the random trees/forest models (`randomForest`), are not usable as they all fail with exhausting the available memory. This means that we are to build an efficient model for the recommendation system almost "from scratch".

Let us now consider the data effects *(O.I)* -- *(O.VII)* outlined in the *"Dataset analysis"* section. The first *(O.I)* -- *(O.V)* effects are the same as observed on the subset of the MovieLens dataset included into the *dslabs* package. In fact this subset was already used for a "Recommendation system" case-study in the literature[^2] and that case study has suggested a statistical model taking into account the user and movie effects along with the Regularization to account *(O.V)*.

Based on the above, we shall split our modeling approach into two parts:

1. **Base model**: We take the statistical model of the movie recommendation system explained in the literature.
2. **Timing model**: We extend the *Base model* taking into account the timing effects *(O.VI)*.

Similar to the timing effects, we could also take the movie popularity effects *(O.VII)* into account. However, as the *"Results"* section will reveal, the *Timing model* will already have the RMSE score significantly below the desired $0.8649$ value, c.f. section *"Project goal"*. Therefore, the *(O.VII)* effect will not be taken into account and will be left for future work.

Further, we shall first outline the **Base model*, as present in the literature. Next, we explain how we extended towards the *Timing model*, taking the timing effects with Regularization into account.

**Note that:** We consider the *Timing model* to be the $\underline{\text{innovative part}}$ of this work, as it builds upon the existing *Base model*, and will be shown to improves the predictions.

[^2]: Chapter $33.7$: *"Recommendation systems"* of the **"Introduction to Data Science"** [online book](https://rafalab.github.io/dsbook/) by Rafael A. Irizarry

## Base model approach


## Modeling timing effects


# Results
<!--a results section that presents the modeling results and discusses the model performance-->

# Conclusions
<!--a conclusion section that gives a brief summary of the report, its limitations and future work (the last two are recommended but not necessary)-->

**TODO: possible extensions** 

* Individual lambda tuning for regularized predictors
* Account for popularity effects
* Account for genres effects
* Account for title effects (sequels, spin-offs, key words)

???
* Check for distribution of the training and validation sets
* Random trees and random forests
* Some standard models
* Cross validation on the training set
* Model ensembles
