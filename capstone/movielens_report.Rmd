---
title: "MovieLens Recommendation System"
author: "Dr. Ivan S. Zapreev"
date: "`r format(Sys.Date())`"
output: pdf_document
---

```{r setup, include=FALSE}
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(graphics)) install.packages("graphics", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")

knitr::opts_chunk$set(echo = TRUE)

library(tidyr)
library(tidyverse)
library(lubridate)
library(graphics)
library(kableExtra)

options(digits=7)

MOVIELENS_DATA_FILE_NAME <- "movielens_data.rda"
MOVIELENS_REPORT_FILE_NAME <- "movielens_report.rda"

#Load the files
load(MOVIELENS_DATA_FILE_NAME)
load(MOVIELENS_REPORT_FILE_NAME)

#Create some shortcuts
edx_set_info <- movielens_report$edx_set_info
valid_set_info <- movielens_report$validation_set_info

#This function is required to image plot the 2D training lambda grid 
#for the individual user/movie effect lambdas Regularization estensions
plot_tuning_grid <- function(tuning_grid, title, lambda_m, lambda_u) {
  #Get the sequences
  lm_seq <- sort(unique(bmilr$model$tuning_grid$lambda_m))
  lu_seq <- sort(unique(bmilr$model$tuning_grid$lambda_u))
  #Convert the grid data frame into a matrix
  mtx <- matrix(0, nrow = length(lm_seq), ncol = length(lu_seq))
  for(i in 1:length(lm_seq)){
    for(j in 1:length(lu_seq)){ 
      rmse <- tuning_grid %>% 
        filter(lambda_m == lm_seq[i] & lambda_u == lu_seq[j]) %>%
        pull(rmse)
      mtx[i,j] <- rmse
    }
  }
  #Visualize the results
  image(lu_seq, lm_seq, t(mtx),
        xlab="User effect lambdas",
        ylab="Movie effect lambdas",
        useRaster=TRUE) %>%
    abline(h=lu_seq+(lu_seq[2]-lu_seq[1])/2, v=lm_seq+(lm_seq[2]-lm_seq[1])/2, col = "grey") %>%
    title(main = title)
  for(i in 1:length(lu_seq)){ 
    for(j in 1:length(lm_seq)){
      if(lambda_m == lm_seq[j] & lambda_u == lu_seq[i]) {
        text(x=lu_seq[i], y=lm_seq[j], as.character(round(mtx[j,i], 7)), cex = 0.625, col = "blue", font=2)
      } else {
        if(i == j) {
          text(x=lu_seq[i], y=lm_seq[j], as.character(round(mtx[j,i], 7)), cex = 0.625, font=2)
        } else {
          text(x=lu_seq[i], y=lm_seq[j], as.character(round(mtx[j,i], 7)), cex = 0.625)
        }
      }
    }
  }
}
```

*[According to Wikipedia](https://en.wikipedia.org/wiki/Recommender_system):*

> A recommender system or a recommendation system (sometimes replacing 'system' with a synonym such as platform or engine) is a subclass of information filtering system that seeks to predict the "rating" or "preference" a user would give to an item.

# Introduction
<!--an introduction/overview/executive summary section that describes the dataset and summarizes the goal of the project and key steps that were performed-->

Recommendation systems are widely used in commercial application to provide targeted commercials or suggest online content. In the latter, one can think of *Spotify* suggesting songs or *Youtube* proposing videos. The main goal of a recommendation system is to accurately predict client's preferences, based on available data on the previous user behavior. Note that, such data includes the behavior of all the clients.

The goal of this work is to build a movie recommendation system based on the `r movielens_report$MOVIELENS_DATA_SET_NAME` available from [`r movielens_report$MOVIELENS_DATA_SET_SITE_URL`](`r movielens_report$MOVIELENS_DATA_SET_SITE_URL`). To reach our goal we will use supervised machine learning techniques studied within the *"PH125.8x Data Science: Machine Learning"* course (a part of the broader HarvardX *Data Science Professional* certification program). The recommendation system will be therefore based on (linear) statistical models trained on the subset of the `r movielens_report$MOVIELENS_DATA_SET_NAME`.

The remainder of this section first present the data set, then defines the goal of this project more concretely, and finally identifies the main steps to be taken to reach the goal.

## Dataset overview
<!--This sub-section stores the references to the original data set and specified the sub-set thereof that is going to be worked with-->

As stated in the [README](http://files.grouplens.org/datasets/movielens/ml-10m-README.html) of the original `r movielens_report$MOVIELENS_DATA_SET_NAME`, it contains `10000054` ratings and `95580` tags (not used in this study) applied to `10681` movies by `71567` users of the [MovieLens](http://www.movielens.org/) service. 

All ratings are contained in the file `ml-10M100K/ratings.dat`. Each line of this file represents one rating specified by the rating value (`rating`), the movie identifier (`movieId`), the identifier of the user (`userId`) and the submission timestamp (`timestamp`). Ratings are made on a `5`-star scale, with half-star increments. Timestamps represent seconds since midnight *Coordinated Universal Time (UTC) of January 1, 1970*. 

Movie information is contained in the file `ml-10M100K/movies.dat`. Each line of this file represents one movie specified by its identifier (`movieId`), movie title (`title`) and the corresponding genres (`genres`). The movie titles are entered manually, so errors and inconsistencies may exist. Genres are pipe-separated lists, of the individual genres from the following set:

  `("Action", "Adventure", "Animation", "Children's", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical", "Mystery", "Romance", "Sci-Fi", "Thriller", "War", "Western")`.
  
Note that, the MovieLens data set is not used "as is" but is pre-processed. The pre-processing steps will be described and explained in the *"Data preparation"* section of this document.

## Project goal
The goal of the project is to build and train a statistical model predicting movie rating based on, at least, the user and the movie. The model is to be trained on the training set (`edx`) and is to be evaluation on a validation set (`validation`).[^1] 

The evaluation of model will be done using the residual mean squared error (RMSE) which, similarly to a standard deviation, can be interpreted as: *the typical error we make when predicting a movie rating*. In other words, e.g, if this number is larger than `1`, it means our typical error is larger than one star.

**Definition:** (`RMSE`)

Let $r_{m,u}$ be the true rating for movie $m$ by user $u$ and $\hat{r}_{m,u}$ be our prediction for that rating, then: 

\[
  RMSE=\sqrt{\sum_{m=1}^{N}(r_{m,u}-\hat{r}_{m,u})^2}
\]

The definition above trivially translates into the following `R` function:

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

```{r, echo=F}
TARGET_RMSE <- 0.8649
```
The ultimate goal of the project is to create a statistical model, solely based on the training set, that on the `validation` set will be able to predict the movie rating with `RMSE` $\leq$ `TARGET_RMSE` = `r TARGET_RMSE`. The way we build such a model will be explained in the *"Modeling approach"* section of this document.

[^1]: These sets are defined in the *"Data preparation"* section.

## Execution plan

Let us now briefly outline the main steps to be performed to reach the previously formalized project goal:

1. **Prepare the data** -- see the *"Data preparation"* section:
    + Select the relevant data; split into training and validation sets; and etc.
2. **Analyze the dataset** -- see the *"Dataset analysis"* section:
    + Perform data exploration and visualization; summarize insights on the data.
3. **Describe the modeling approach** -- see the *"Modeling approach"* section:
    + Consider the insights of the data analysis; suggest the way for building the prediction model.
4. **Present modeling results** -- see the  *"Results"* section:
    + Train the model on the `edx` set; analyze the training results; evaluate on the `validation` set.
5. **Provide concluding remarks** -- see the *"Conclusions"* section:
    + Summarize the results; mention any approach limitations; outline possible future improvements.

Please note that, the remainder of the document is structured according to the execution plan listed above.

# Data preparation
<!--This section explains how the original MovieLens data set is pre-processed, cleaned and split to get the training and validation sets for training and validation of the statistical model(s) to be build-->

In this project, we use a pre-processed subset of the original dataset. Let us now explain how that subset obtained and pre-processed. Here we only use the data from the `ratings.dat` and `movies.dat` files from the `r movielens_report$MOVIELENS_DATA_SET_NAME`. To facilitate supervised learning, the data is pre-processed and split into a training (`edx`) and testing (`validation`) sets. The former will be used for training statistical model(s) and the latter will be used for the model(s) validation.

We pre-process and split the data in the following way:

1. The `ratings.dat` and `movies.dat` are loaded into `ratings` and `movies` data frames
    1. `ratings` -- with columns named `r c("userId", "movieId", "rating", "timestamp")`
    2. `movies` -- columns named `r c("movieId", "title", "genres")`
2. The ratings are coupled with the movies information by the `movieId` into a new `movielens` data frame
3. The `movielens` data frame is randomly split into two parts:
    2. `edx` -- a set storing the `r (1-movielens_report$VALIDATION_TO.EDX_SET_RATIO)*100`% of the observations from the `movielens`, to be used for training
    1. `validation` -- a set storing the `r movielens_report$VALIDATION_TO.EDX_SET_RATIO*100`% of the observations from the `movielens`, to be used for validation
4. The `validation` set is filtered to only contain movies and users present in `edx`
5. The observations filtered out in the previous step are added back to the `edx` set

For more details, see the `create_movielens_sets` function located in the `movielens_project.R` script. As a result of pre-processing we have two sets with the following metrics, the training set `edx`:

```{r, echo=F}
as_tibble(edx_set_info)
```

and the validation set `validation`:

```{r, echo=F}
as_tibble(valid_set_info)
```

```{r, echo=F}
percent_obs_edx <- round(edx_set_info$num_observations/(edx_set_info$num_observations+valid_set_info$num_observations)*100)
```
As one can see the `edx` set stores `r percent_obs_edx`% of the total number of observations (`num_observations`) and the `validation` set contains `r 100-percent_obs_edx`%. The difference in the number of distinct movies (`num_movies`) and the number of distinct users (`num_users`) is explained by the observations being moved back from `validation` to `edx` in the last data preprocessing step, described above.

Given the training and validation set preparation procedure above we assume that the resulting sets follow the same probability distribution of the conditional probability of interest, see the section on the *"Modeling approach"*.

At last, let us mention that the `edx` and `validation` set analysis revealed no `N/A` values are present and also there was no need detected to do any additional cleaning of the data.

# Dataset analysis
<!--In this section we analyze the training set, by means of data exploration and visualization, in order to get insights on the data and to highlight its features that could be further used in developing the prediction model-->

In this sections we analyze the training (`edx`) subset of the original MovieLens data set only, as we assume that this is the only data available for building the prediction models. The motivation behind is that we want to avoid influencing the model validation results. This is done under a, not-verified, assumption that the the testing set, follows the same probability distribution of the conditional probability of interest as the validation set. In the *"Results"* section we may come back to this assumption in case the accuracy of the developed statistical model(s) on the validation set will turn out to be insufficient.

From the number of distinct user, movies and observations (ratings) in the `edx` set it is clear that not every user has rated every movie as `r edx_set_info$num_users` \* `r edx_set_info$num_movies` < `r edx_set_info$num_observations`. The same can be seen via the next plot for randomly sampled `100` unique users and `100` unique movies:

```{r, echo=F}
#Sample 100 individual movie ids
movie_ids <- unique(movielens_data$edx$movieId)
sample_movie_ids <- sample(movie_ids, 100)

#Plot the sparse matrix
movielens_data$edx %>%
  filter(movieId %in% sample_movie_ids) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 100) %>%
  spread(userId, rating) %>%
  select(sample(ncol(.), 100)) %>% #Here we sample a 100 individual user ids
  as.matrix() %>%
  t(.) %>%
  image(1:100, 1:100, . , xlab="Users", ylab="Movies") %>%
  abline(h=0:100+0.5, v=0:100+0.5, col = "grey") %>%
  title(main = "Movie ratings sparsity")
```

The the task of a recommendation system we are to build can be seen as filling in the missing ratings.
To stress the complexity of the undertaking, below we list a number of different observations that can influence rating predictions, and may need to be taken into account when building the statistical model.

**(O.I):** Some movies are rated more often than the others:

```{r}
movielens_data$edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  labs(x = "Number of ratings", y = "Number of movies") + 
  ggtitle("Movie rating counts") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.II):** Some users are more active in rating movies that the others:

```{r}
movielens_data$edx %>%
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() +
  labs(x = "Number of ratings", y = "Number of users") + 
  ggtitle("User rating activity") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.III):** Movies differ from each other in given ratings, lets consider `10` movies with at least `100` ratings each:

```{r}
#Sample 10 individual movie ids for the movies with more than 100 ratings
movie_ids <- movielens_data$edx %>%
  group_by(movieId) %>% summarise(cnt = n()) %>% 
  filter(cnt > 100) %>% pull(movieId) %>% unique(.)
set.seed(1)
sample_movie_ids <- sample(movie_ids, 10)

#Use the box plot to show variations in ratings between different movies
movielens_data$edx %>% 
  filter(movieId %in% sample_movie_ids) %>% 
  ggplot(aes(title, rating, group=movieId)) +
  geom_boxplot() +
  scale_x_discrete(labels = paste("movieId ",as.character(sample_movie_ids)))+
  theme(axis.text.x = element_text(angle = 20)) + 
  labs(x = "Selected movies", y="Received ratings") + 
  ggtitle("Movie effects") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.IV):** Users differ from each other in giving ratings, lets consider `10` users with at least `100` ratings each:

```{r}
#Sample 10 individual user ids for the users with more than 100 ratings
user_ids <- movielens_data$edx %>%
  group_by(userId) %>% summarise(cnt = n()) %>% 
  filter(cnt > 100) %>% pull(userId) %>% unique(.)
set.seed(5)
sample_user_ids <- sample(user_ids, 10)

#Use the boxplot to show variations in ratings between different users
movielens_data$edx %>% 
  filter(userId %in% sample_user_ids) %>% 
  mutate(userName = paste("userId ", as.character(userId))) %>%
  group_by(userId) %>%
  ggplot(aes(userName, rating, group=userId)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 20)) +
  labs(x = "Selected users", y= "Given ratings") + 
  ggtitle("User effects") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.V):** The best and the worst (on-average) movies were not rated very often:

```{r, echo=F}
#Per movie, get the average movies score and rating counts
avg_movie_ratings <- movielens_data$edx %>%
  group_by(movieId) %>%
  summarize(avg_rating = mean(rating),
            num_ratings = n()) %>%
  arrange(desc(avg_rating))
```

```{r}
#Get the movies with the top 10 average scores
true_top_10 <- avg_movie_ratings %>%
  arrange(desc(avg_rating)) %>% 
  slice(1:10)
true_top_10

#Get the movies with the bottom 10 average scores
true_bottom_10 <- avg_movie_ratings %>%
  arrange(avg_rating) %>% 
  slice(1:10)
true_bottom_10
```

See also the next plot where the best movies are marked as blue and the worst as red:
```{r}
#Plot the movies
avg_movie_ratings %>%
  ggplot(aes(x = num_ratings, y = avg_rating)) +
  geom_point(aes(color = I("black")), alpha=0.2)+
  geom_point(data = true_top_10, aes(num_ratings, avg_rating), color="blue") +
  geom_point(data = true_bottom_10, aes(num_ratings, avg_rating), color="red") +
  labs(x="Number of movie ratings", y="Average movie rating") +
  ggtitle("Best and worst movie effects") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.VI):** The rating scores depend on the time when the rating is given:

```{r}
#Convert the timestamp into a week (date) and then plot 
#the mean rating for all the movies per week
movielens_data$edx %>%
  mutate(date = as_datetime(timestamp), week = round_date(date, "weekId")) %>%
  group_by(weekId) %>%
  summarize(avg_rating = mean(rating)) %>%
  ggplot(aes(weekId, avg_rating)) +
  geom_line() +
  geom_smooth(method = 'loess', method.args=list(degree=2)) +
  labs(x="Week", y="Average rating") +
  ggtitle("Global timing effects") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

**(O.VII):** The ratings show dependency from the movie's popularity is rated:

As we already see from the plot above, there is more variability for the movies with fewer ratings. Let us now consider the same plot with the smoothened average movie rating trend line:
```{r}
#Plot the movies
avg_movie_ratings %>%
  ggplot(aes(x = num_ratings, y = avg_rating)) +
  geom_point(aes(color = I("black")), alpha=0.2) +
  geom_smooth(method = 'loess', method.args=list(degree=2)) +
  labs(x="Number of movie ratings", y="Average movie rating") +
  ggtitle("Movie popularity effects") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
We can clearly see that the movies that are rated more often (we call them "popular" movies) are rated higher on-average.

In addition we could consider the effects caused by the movie titles and genres. One could think of analyzing the influence of:

* Keywords and phrases in the movie titles
* Connection between sequels, prequels, and spin-offs
* Movie genre and genre combinations

Although possible and interesting, due to the lack of time, the latter will not be considered in this study.

# Modeling approach
<!--This section describes the modeling approach as based on the insights obtained in the previous section-->

Consider the columns of the available MovieLens data:
```{r, echo=F}
names(movielens_data$edx)
```
Then the conditional probability distribution that is to be estimates can be formalized as:
\[
Pr\left(R \; \vert \; M = m, \;U = u, \;T = t, \;G = g, \;L = l\right)
\]
Here the movie-related random variables are:

* $R$ -- the rating to be given; 
* $U$ -- the user id; 
* $M$ -- the movie id; 
* $T$ -- the ratings timestamp; 
* $G$ -- the movie genres; 
* $L$ -- the movie title; 

So here we could use $5$ predictors do provide the estimates of $R$.

As discussion in the end of the *"Dataset analysis"* section, we shall NOT consider the effects caused by the genres ($G$) and movie titles ($T$). This leaves us with three predictors $U$, $M$ and $T$ and the conditional probability distribution to be estimated:
\[
Pr\left(R \; \vert \; M = m, \;U = u, \;T = t\right)
\]

Before we proceed, let us note that the dimension reduction techniques, e.g. Principal Component Analysis (PCA), are not very useful here as the number of predictors is small, so they will not be used. Moreover, due to the large amount of data in the dataset, the standard available statistical models in $R$, such as the Linear Model (`lm`) or the random trees/forest models (`randomForest`), are not usable as they all fail with exhausting the available memory. This means that we are to build an efficient model for the recommendation system almost "from scratch".

Let us now consider the data effects *(O.I)* -- *(O.VII)* outlined in the *"Dataset analysis"* section. The first *(O.I)* -- *(O.V)* effects are the same as observed on the subset of the MovieLens dataset included into the *dslabs* package. In fact this subset was already used for a "Recommendation system" case-study in the literature[^2] and that case study has suggested a statistical model taking into account the user and movie effects along with the Regularization to account *(O.V)*.

Based on the above, we shall split our modeling approach into two parts:

1. **Base model**: The movie recommendation system explained in the literature.
2. **Timing model**: We extend the *Base model* taking into account the timing effects *(O.VI)*.

Similar to the timing effects, we could also take the movie popularity effects *(O.VII)* into account. However, as the *"Results"* section will reveal, the *Timing model* will already have the RMSE score on the `validation` set below the desired $0.8649$ value, c.f. section *"Project goal"*. Therefore, the *(O.VII)* effect will not be taken into account and will be left for future work.

Further, we shall first outline the *Base model*, as present in the literature. Next, we explain how we extended towards the *Timing model*, taking the timing effects into account.

**Note that:** We consider the *Timing model* to be the **first** $\underline{\text{innovative part}}$ of this work, as it builds upon the existing *Base model*, and will be shown to improve predictions. The **second** $\underline{\text{innovative part}}$ will be attempting using different Regularization parameters for the user and movie effects. This is a natural extension as one can suspect that movie and user effects may be required to be penalized differently.

[^2]: Chapter $33.7$: *"Recommendation systems"* of the **"Introduction to Data Science"** [online book](https://rafalab.github.io/dsbook/) by Rafael A. Irizarry

## Base model
The *Base model* estimates the probability distribution of $Pr\left(R \; \vert \; M = m, \;U = u\right)$ and can be summarized as:
\[
    R_{m,u} = \mu + b_{m} + b_{u} + \epsilon_{m,u}
\]
where:

* $\epsilon_{m,u}$ -- independent errors, sampled from the same distribution centered at $0$
* $\mu$ -- the average rating of all the movies
* $b_{m}$ -- the movie bias for movie $m$
* $b_{u}$ -- the user bias for user $u$

Note that, $b_{m}$ and $b_{u}$ could be estimated using, for instance, the linear model (using `lm()` in `R`):
```{r, eval=FALSE}
lm(rating ~ as.factor(movieId) + as.factor(userId))
```
However there are two reasons for not doing this:

1. Due to the size of the MovieLens data set and the number of individual movies and users, the time needed to fit even this simple model on a regular PC can be days.
2. To account for the *(O.V)* effects, we will use Regularization that has to penalize the $b_{m}$ and $b_{u}$ values.

The Regularization technique, with the $\lambda$ modeling parameter, is used to account for the movies for which too few ratings are given and the users that gave too few ratings. Those by default may get $b_{m}$ and $b_{u}$ values which are either too high or too low solely due to being based on too few observations. The base model approach therefore suggests to use the estimated $\mu$, $b_{m}$ and $b_{u}$ values as follows:
\[
    \hat{\mu} = \frac{1}{N}\sum_{m,u}r_{m,u}
\]
\[
    \hat{b_{m}} = \frac{\sum_{u=1}^{N_{m}}\left(r_{m,u} - \hat{\mu}\right)}{\lambda + N_{m}}
\]
\[
    \hat{b_{u}} = \frac{\sum_{m=1}^{N_{u}}\left(r_{m,u} - \hat{b_{m}} - \hat{\mu}\right)}{\lambda + N_{u}}
\]
where:

* $r_{m,u}$ -- is the rating of the movie $m$ by the user $u$
* $N$ -- is the total number of movie ratings
* $N_{m}$ -- is the number of ratings given to the movie $m$
* $N_{u}$ -- is the number of ratings given by the user $u$

The optimal value of $\lambda$ is to be selected during the training phase of the model. The `RMSE` optimizing value of $\lambda_{\text{opt}}$ is to be used when computing the `RMSE` scores but the value of $\lambda_{\text{opt}}$ can not be based on the `validation` set. Thus, we apply cross validation technique by splitting the `edx` set into the `training` and `testing` sets respectively taking `r (1 - movielens_report$TEST_TO_TRAIN_SET_RATIO)*100`% and `r movielens_report$TEST_TO_TRAIN_SET_RATIO*100`% of the `edx` set. To avoid testing failures, the care is taken of not having users and movies in the `testing` sets that are not present in the `training` set.

The actual *Base model* movie rating estimate for movie $m$ and user $u$ will then be:
\[
    r_{m,u} = \hat{\mu} + \hat{b_{m}} + \hat{b_{u}}
\]

## Timing model
To account for the timing effects, we need to estimate the probability distribution of 
\[
Pr\left(R \; \vert \; M = m, \;U = u, \;T = t\right)
\]
which can be done using the next model:
\[
    R_{m,u} = \mu + b_{m} + b_{u} + f\left(t_{m,u}\right) + \epsilon_{m,u}
\]
or equivalently
\[
    R_{m,u} = \mu + b_{m} + b_{u} + b_{t} + \epsilon_{m,u}
\]
where:

* $t_{m,u}$ -- is the time at which the movie $m$ is rated by the user $u$
    + We shall stratify our time per week, as was done to show the *(O.V)* effect
* $f\left(.\right)$ -- a smooth function of its argument approximating the average movie ratings per week
    + We shall use the Local Polynomial Regression Fitting (*LOESS*) with the degree $2$ for this
* $b_{t}$ -- is the correction of the mean $\mu$ by the mean rating deviation from $\mu$ in week $t = t_{m,u}$
    
As with the *Basic model* the $b_{m}$, $b_{u}$, and $b_{t}$ coefficients are to be estimated from the data. 

Let us first explain how the $b_t$ values are computed:

```{r, eval=F}
    #Compute the smooth Local Regression (LOESS)
    #fit for the average movie rating per week
    amrpw_fit <- train_set %>% 
      group_by(weekId) %>%
      summarise(avg_rating = mean(rating)) %>%
      mutate(weekId = as.numeric(weekId)) %>%
      loess(avg_rating ~ weekId, degree = 2, data = .)

    #Compute the movie rating timing effects per week:
    b_ts <- train_set %>% 
      mutate(amrpw = predict(amrpw_fit, as.numeric(weekId))) %>%
      group_by(weekId) %>%
      summarize(b_t = min(amrpw) - mu_hat)
```
I.e. the smooth function $f\left(.\right)$ is estimated as the polynomial surface based on the average movie rating per week, using the LOESS algorithm. The $b_t$ values are then computed as the "smoothened average movie rating in week $t$", i.e. $f\left(t\right)$, minus the "overall average movie rating", given by $\hat{\mu}$.

To conclude the movie rating estimate of the *Timing model* for movie $m$ and user $u$ in week $t$ is:
\[
    r_{m,u} = \hat{\mu} + \hat{b_{t}} + \hat{b_{m}} + \hat{b_{u}}
\]
with the coefficients computed as follows:
\[
    \hat{\mu} = \frac{1}{N}\sum_{m,u}r_{m,u}
\]
\[
    \hat{b_{t}} = f\left(b_{t}\right) - \hat{\mu}
\]
\[
    \hat{b_{m}} = \frac{\sum_{u=1}^{N_{m}}\left(r_{m,u} - \hat{b_{t}} - \hat{\mu}\right)}{\lambda + N_{m}}
\]
\[
    \hat{b_{u}} = \frac{\sum_{m=1}^{N_{u}}\left(r_{m,u} - \hat{b_{t}} - \hat{b_{m}} - \hat{\mu}\right)}{\lambda + N_{u}}
\]

Please note that, since we do not apply Regularization to the $\hat{b_{t}}$ parameter, the clarification for the $\hat{b_{m}}$ and $\hat{b_{u}}$ formulae above is straightforward. We can obtain these by considering the  *Basic model* and assume that instead of the overall mean rating $\hat{\mu}$ we use the weekly mean rating $\hat{\mu} + \hat{b_{t}}$. Then the $\hat{b_{m}}$ and $\hat{b_{u}}$ then we will get the corresponding formulae of the *Timing model*.

## Ext. models

We shall also consider the *Basic Ext.* and *Timing Ext.* models that differ from the original only in that they use individual lambdas tuning for user and movie effects in Regularization. In this case, the $\hat{b_{m}}$ and $\hat{b_{u}}$ formulae used in the *Basic* and *Timing* models will be accordingly altered as follows:
\[
    \hat{b_{m}} = \frac{\sum_{u=1}^{N_{m}}\left(r_{m,u} - \hat{\mu}\right)}{\lambda^{m} + N_{m}}
\]
\[
    \hat{b_{u}} = \frac{\sum_{m=1}^{N_{u}}\left(r_{m,u} - \hat{b_{m}} - \hat{\mu}\right)}{\lambda^{u} + N_{u}}
\]
and
\[
    \hat{b_{m}} = \frac{\sum_{u=1}^{N_{m}}\left(r_{m,u} - \hat{b_{t}} - \hat{\mu}\right)}{\lambda^{m} + N_{m}}
\]
\[
    \hat{b_{u}} = \frac{\sum_{m=1}^{N_{u}}\left(r_{m,u} - \hat{b_{t}} - \hat{b_{m}} - \hat{\mu}\right)}{\lambda^{u} + N_{u}}
\]
The parameter tuning will be then done on a two dimensional grid to find a pair of RMSE optimal lambda values $\lambda^{m}_{opt}$ and  $\lambda^{u}_{opt}$.

# Results
<!--a results section that presents the modeling results and discusses the model performance-->
Below we present our model tuning and validation results. The former indicate the found optimal Regularization parameter $\lambda$ values. The latter provide the trained model `RMSE` scores on the `validation` set. We start with presenting the *Basic model* results and then continue with those of the *Timing model*. The results for the model extension when tuning with individual lambdas are provided in between. In the end, we also summarize the obtained results in the *"Results summary"* subsection.

## Basic model
```{r, echo=F}
bmr <- movielens_report$basic_model_res
lm_min <- min(bmr$model$tuning_grid$lambda)
lm_max <- max(bmr$model$tuning_grid$lambda)
bmr_bt_rmse <- min(bmr$model$tuning_grid$rmse)
bmr_lam <- bmr$model$lambda_m
```
Training the *Basic model* on the range of $\lambda$ in [`r lm_min`, `r lm_max`] has revealed that the optimal value is $\lambda_{opt} =$ `r bmr_lam`, with the corresponding minimum `RMSE` score on the `testing` set `r bmr_bt_rmse`, see the training plot below:
```{r, echo = F}
min_rmse <- min(bmr$model$tuning_grid$rmse)
max_rmse <- max(bmr$model$tuning_grid$rmse)
tibble(lambdas = bmr$model$tuning_grid$lambda,
       rmses = bmr$model$tuning_grid$rmse) %>%
  ggplot(aes(lambdas, rmses)) +
  geom_point() +
  geom_hline(aes(yintercept = min_rmse, color=I("brown")), lty=2) +
  scale_y_continuous(breaks=seq(min_rmse, max_rmse, length.out = 10)) +
  scale_x_continuous(breaks=seq(0, 7, 0.5)) +
  theme(axis.text.x = element_text(angle = 45)) + 
  labs(x="lambda", y="RMSE") +
  ggtitle("Basic model fitting on the testing set") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
The `RMSE` score of the *Basic model* on the `validation` set is `r bmr$rmse`.

### Basic Ext. model
```{r, echo=F}
bmilr <- movielens_report$basic_model_ind_lam_res
lm_min <- min(bmilr$model$tuning_grid$lambda_m)
lm_max <- max(bmilr$model$tuning_grid$lambda_m)
lu_min <- min(bmilr$model$tuning_grid$lambda_u)
lu_max <- max(bmilr$model$tuning_grid$lambda_u)
bmilr_bt_rmse <- min(bmilr$model$tuning_grid$rmse)
bmilr_lam_m <- bmilr$model$lambda_m
bmilr_lam_u <- bmilr$model$lambda_u
```
Training the *Basic model* on the range of $\left(\lambda^{m},\;\lambda^{u}\right)$ in [`r lm_min`, `r lm_max`]x[`r lu_min`, `r lu_max`] has revealed that the optimal values are $\lambda^{m}_{opt} =$ `r bmilr_lam_m` and $\lambda^{u}_{opt} =$ `r bmilr_lam_u`, with the corresponding minimum `RMSE` score on the `testing` set `r bmilr_bt_rmse`.

```{r, echo=F}
plot_tuning_grid(bmilr$model$tuning_grid,
                 "Basic model lambda tuning grid",
                 bmilr$model$lambda_m,
                 bmilr$model$lambda_u)
```

As one can see the found optimal lambda values are not equal to each other and the `RMSE` score on the `testing` set (`r bmilr_bt_rmse`) is lower than that for the original *Basic model* (`r bmr_bt_rmse`).

The `RMSE` score on the `validation` set turns out to be `r bmilr$rmse`, that is also smaller than the original `RMSE` of the *Basic model* (`r bmr$rmse`).

## Timing model
```{r, echo=F}
tmr <- movielens_report$timing_model_res
lm_min <- min(tmr$model$tuning_grid$lambda)
lm_max <- max(tmr$model$tuning_grid$lambda)
tmr_bt_rmse <- min(tmr$model$tuning_grid$rmse)
tmr_lam <- tmr$model$lambda_m
```
Training the *Timing model* on the range of $\lambda$ in [`r lm_min`, `r lm_max`] has revealed that the optimal value is $\lambda_{opt} =$ `r tmr_lam`, with the corresponding minimum `RMSE` score on the `testing` set `r tmr_bt_rmse`, see the training plot below:
```{r, echo = F}
min_rmse <- min(tmr$model$tuning_grid$rmse)
max_rmse <- max(tmr$model$tuning_grid$rmse)
tibble(lambdas = tmr$model$tuning_grid$lambda,
       rmses = tmr$model$tuning_grid$rmse) %>%
  ggplot(aes(lambdas, rmses)) +
  geom_point() +
  geom_hline(aes(yintercept = min_rmse, color=I("brown")), lty=2) +
  scale_y_continuous(breaks=seq(min_rmse, max_rmse, length.out = 10)) +
  scale_x_continuous(breaks=seq(0, 7, 0.5)) +
  theme(axis.text.x = element_text(angle = 45)) + 
  labs(x="lambda", y="RMSE") +
  ggtitle("Timing model fitting on the testing set") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
The `RMSE` score of the *Timing model* on the `validation` set is `r tmr$rmse`.

### Timing Ext. model
```{r, echo=F}
tmilr <- movielens_report$timing_model_ind_lam_res
lm_min <- min(tmilr$model$tuning_grid$lambda_m)
lm_max <- max(tmilr$model$tuning_grid$lambda_m)
lu_min <- min(tmilr$model$tuning_grid$lambda_u)
lu_max <- max(tmilr$model$tuning_grid$lambda_u)
tmilr_bt_rmse <- min(tmilr$model$tuning_grid$rmse)
tmilr_lam_m <- tmilr$model$lambda_m
tmilr_lam_u <- tmilr$model$lambda_u
```
Training the *Timing model* on the range of $\left(\lambda^{m},\;\lambda^{u}\right)$ in [`r lm_min`, `r lm_max`]x[`r lu_min`, `r lu_max`] has revealed that the optimal values are $\lambda^{m}_{opt} =$ `r tmilr_lam_m` and $\lambda^{u}_{opt} =$ `r tmilr_lam_u`, with the corresponding minimum `RMSE` score on the `testing` set `r tmilr_bt_rmse`.

```{r, echo=F}
plot_tuning_grid(tmilr$model$tuning_grid,
                 "Timing model lambda tuning grid",
                 tmilr$model$lambda_m,
                 tmilr$model$lambda_u)
```

As one can see the found optimal lambda values are not equal to each other and the `RMSE` score on the `testing` set (`r tmilr_bt_rmse`) is lower than that for the original *Timing model* (`r tmr_bt_rmse`).

The `RMSE` score on the `validation` set turns out to be `r tmilr$rmse`, that is somewhat higher than the original `RMSE` of the *Timing model* (`r tmr$rmse`).

## Results summary
<!-- Provides a brief summary of the experimental results -->
Let us summarize and discuss the training and validation results of the models considered above:
```{r, echo=F}
tbl <- tibble( Models = c("Basic", "Basic Ext.", "Timing", "Timing Ext."),
               lambda = c(as.character(bmr_lam), "---", as.character(tmr_lam), "---"),
               lambda_m = c("---", as.character(bmilr_lam_m), "---", as.character(tmilr_lam_m)),
               lambda_u = c("---", as.character(bmilr_lam_u), "---", as.character(tmilr_lam_u)),
               rmse_train = c(bmr_bt_rmse, bmilr_bt_rmse, tmr_bt_rmse, tmilr_bt_rmse),
               rmse_validate = c(bmr$rmse, bmilr$rmse, tmr$rmse, tmilr$rmse),
               imp = c((TARGET_RMSE - bmr$rmse),
                          (bmr$rmse - bmilr$rmse),
                          (bmilr$rmse - tmr$rmse),
                          (tmr$rmse - tmilr$rmse)))
kable(tbl, caption = "Training and validation results",
      col.names = c("Models", "$\\lambda_{opt}$", "$\\lambda^{m}_{opt}$",
                    "$\\lambda^{u}_{opt}$", "RMSE", "RMSE", "Improvement"),
      escape = FALSE, align=rep('c', 7)) %>%
  add_header_above(c(" " = 1, "Training" = 4, "Validation" = 2)) %>%
  kable_styling(latex_options =c("striped", "hold_position"))
```
The table above lists the suggested MovieLens models, with the corresponding information on the:

* Training:
    * The optimal $\lambda$ values for Regularization
    * The `RMSE` scores found on the `testing` set
* Validation:
    * The `RMSE` score of the model on the `validation` set
    * The model's its improvement relative to the previous model
        * The *Base model* improvement is relative to the `TARGET_RMSE` (`r TARGET_RMSE`)

From this table we can observe that:

1. All of the considered models *improve* on the `TARGET_RMSE`
2. The *innovative Timing model* gives a significant improvement over the *Base model*
3. The effect of the *innovative Ext. models* is inconclusive:
    1. Using individual lambdas allow for a better `RMSE` on the `testing` set
        + Consider *Base* vs. *Base Ext.*  models *showing improvement*
        + Consider *Timing* vs. *Timing Ext.*  models *showing improvement*
    2. The better `testing` set `RMSE` does not always imply a better `validation` set `RMSE`
        + Consider the *Base* vs. *Base Ext.* models *showing improvement*
        + Consider the *Timing* vs. *Timing Ext.* models *showing deterioration*

The best scoring model relative to the `TARGET_RMSE` = `r TARGET_RMSE` is the *Timing model* with `RMSE` = `r round(tmr$rmse, 4)`.

# Conclusions
<!--A conclusion section that gives a brief summary of the report, its limitations 
    and future work (the last two are recommended but not necessary)-->
In this report we have presented our approach to building a movie recommendation system based on the `r movielens_report$MOVIELENS_DATA_SET_NAME` available from [`r movielens_report$MOVIELENS_DATA_SET_SITE_URL`](`r movielens_report$MOVIELENS_DATA_SET_SITE_URL`), employing the supervised machine learning techniques studied within the *"PH125.8x Data Science: Machine Learning"* course.

In this work we have first provided an overview of the data set at hand, then summarized the project goal, and listed the execution plan. Next, we have explained the way the data set was prepared and cleaned. Further, we went on with the data analysis that allowed us to summarize seven key observations (*O.I* -- *O.VII*) about the data at hand. In particular, we have discovered that the ratings are affected by the user, movie, and timing effects; and that we do need to employ Regularization to correct for the inaccuracies introduced by the movies and users with a very few ratings.

The literature analysis has revealed that Chapter $33.7$: *"Recommendation systems"* of the **"Introduction to Data Science"** [online book](https://rafalab.github.io/dsbook/) by Rafael A. Irizarry, already provides a statistical model for the movie recommendation system that accounts for the first 5 of the discovered effects (*O.I* -- *O.V*). We have then decided to employ that model as the *Base model* for this work.

Further, we went on with the modeling section and have started it with explaining the *Base model* in details. We have then suggested an *innovative extension* of this model, to account for the timing effect (*O.VI*), and called this new model the *Timing model*. In addition, *another innovation* was suggested, namely to use the individual lambdas for movie and user effects in Regularization. This has resulted in doubling the number of models under study, the additional models were: *Base Ext. model* and *Timing Ext. model*.

All of the suggested models have been implemented (by means of an `R` script), trained (on the specified `edx` set) and validated (on the specified `validation` set). In the process of training, we have also used model cross validation for the sake of Regularization parameter tuning on an independent set of data.

Our experiments with the models at hand have revealed that:

* All of the model have a better `RMSE` scores that the specified `TARGET_RMSE` (`r TARGET_RMSE`).
* The *Timing model* had the best `RMSE` score (`r tmr$rmse`) of all the models.
* The effect of the *Ext.* models was inconclusive:
    * It was shown that tuning individual lambdas improves the `RMSE` score on the `testing` set
    * A better `testing` set `RMSE` score however did not always imply a better `validation` set `RMSE`
    
All in all, the goals of the project have been met, both in regards of beating the `TARGET_RMSE` score and suggesting and innovative statistical models for the movie recommendation system.

## Future work
In conclusion we shall say that, there are still a number of effects that could be accounted for to further improve our *Timing model*. For instance, we could take into account the movie popularity (*O.VII*), movie genres, and movie title (sequels, spin-offs, key words) effects. Moreover, we have observed that standard statistical models provided with `R` have failed on the MovieLens data set due to its immense size. However, we believe that perhaps a more efficient re-implementation of such models: `LM`, `LOESS`, `randomForest`, and etc. could allow us to improve our recommendation system even further by means of combining them, and the provided *Timing model*, into model ensembles.
