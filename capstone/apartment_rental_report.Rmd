---
title: "Apartment Rental Prediction System"
author: "Dr. Ivan S. Zapreev"
date: "`r format(Sys.Date())`"
output: pdf_document
number_sections: true
toc: true
---

```{r setup, include=FALSE}

if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(graphics)) install.packages("graphics", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(tidyr)
library(lubridate)
library(ggplot2)
library(caret)
library(dslabs)
library(data.table)
library(dplyr)
library(knitr)
library(graphics)
library(kableExtra)

knitr::opts_chunk$set(echo = TRUE)

options(digits=7)

MOVIELENS_DATA_FILE_NAME <- "arog_data.rda"
MOVIELENS_REPORT_FILE_NAME <- "arog_report.rda"

#Load the files
load(MOVIELENS_DATA_FILE_NAME)
load(MOVIELENS_REPORT_FILE_NAME)

#--------------------------------------------------------------------
# This function counts the number of N/A entries in the columns of 
# the provided data set, it returns a tibble with the columns:
#    name - storing the column name of the data set
#    count - storing the number of N/A entries
#    percent - storing the percent of N/A entries
#--------------------------------------------------------------------
count_na_entries <- function(data_set) {
  col_names <- names(data_set)
  na_counts <- as.vector(sapply(col_names, function(name){
    return(sum(is.na(data_set[,name])))
  }))
  num_rows <- nrow(data_set)
  percents <- round(na_counts*100/num_rows, 2)
  result <- tibble(name = col_names, 
                count = na_counts,
                percent = percents) %>%
    arrange(desc(count))
  names(result) <- c("Column name", "N/A count", "N/A percent")
  return(result)
}

#--------------------------------------------------------------------
# This function is used to make the number of floors summary for an
# apartment types. It accepts an apartment type and a data set, which
# defaults to arog_data$selected_data, and then returns a tibble, with 
# the following data for the flats of the app_type type:
#    app_type - apartment type
#    count - the number of non N/A numberOfFloors
#    avg - average of numberOfFloors values
#    se - standard error of numberOfFloors values
#    min - min numberOfFloors value
#    max - max numberOfFloors value
#--------------------------------------------------------------------
nr_floors_summary <- function(app_type, data_set = arog_data$selected_data) {
  df <- data_set %>%
    filter(!is.na(numberOfFloors) & (typeOfFlat == app_type)) %>% 
    summarise(app_type = app_type,
              count = n(),
              avg = round(mean(numberOfFloors), 1), 
              se = round(sd(numberOfFloors), 1),
              min = min(numberOfFloors),
              max = max(numberOfFloors))
  return(as_tibble(df))
}

#--------------------------------------------------------------------
#This function fixes the mean statistics with the log10 scale used in ggplot
#--------------------------------------------------------------------
log10_mean <- function(x) {
  log10(mean(10 ^ x)) 
}
```

# Introduction
<!-- An introduction/overview/executive summary section that:
 1. Describes the dataset
 2. Summarizes the goal of the project 
 3. Outlines the key steps that were performed; -->

In this report we shall address building of an apartment rental price prediction system. The latter will be based on the `r arog_report$AROG_DATA_SET_NAME` available from:

[`r arog_report$AROG_DATA_SET_SITE_URL`](`r arog_report$AROG_DATA_SET_SITE_URL`).

To reach our goal, we will use supervised machine learning techniques studied within the *"PH125.8x Data Science: Machine Learning"* course (a part of the broader HarvardX *Data Science Professional* certification program). The prediction system will be therefore based on (linear) statistical models trained on the subset of the `r arog_report$AROG_DATA_SET_NAME`.

The remainder of this section first present the data set, then defines the goal of this project more concretely, and finally identifies the main steps to be taken to reach the goal.

## Dataset overview

As stated on the [webpage](`r arog_report$AROG_DATA_SET_SITE_URL`) of the `r arog_report$AROG_DATA_SET_NAME`, it contains `198,379` rental offers scraped from the Germany's biggest real estate online platform ÃŸ [ImmobilienScout24](https://www.immobilienscout24.de/). 

The data set consists of a single CSV file: *`r arog_report$AROG_CSV_FILE_NAME`* which only contains offers for rental properties. The data features important rental property attributes, such as the living area size, the rent (both base rent as well as total rent), the location, type of energy, and etc. The `date` column present in the data set defines the time of scraping, which was done on three distinct dates: *2018-09-22*, *2019-05-10* and *2019-10-08*.

The complete list of data set columns is extensive[^1] and thus in this study we will use the following subset:
```{r, echo=F}
names(arog_data$selected_data)
```
This sub-selection reduces the number of considered data set columns[^2] from $48$ to $26$ and is motivated by the personal preferences of the report's author and has no scientifically proven motivation. On the contrary, this column selection shall be seen as a part of problem statement. In other words, the task is to build an accurate[^3] rental price prediction model based on the predictors from this set of columns.

The additional data preparation steps will be described in the *"Data wrangling"* section of this document.

[^1]: Please consider reading *"Appendix A"* for the complete list of the data set columns.
[^2]: Please consider reading *"Appendix B"* for the column descriptions.
[^3]: Please consider reading the *"Project goal"* section for an exact goal formulation.

## Project goal
The goal of this project is to build a apartment rental-price prediction system for German cities. The model is to be trained and validated using the data from `r arog_report$AROG_DATA_SET_NAME`. The data is therefore to be split into a `modeling` and `validation` sets which will be defined in the *"Data wrangling"* section.

The system evaluation will be done using the Residual Mean Squared error (`RMSE`) which, similarly to a standard deviation, can be interpreted as: *the typical error we make when predicting a rent price*. In other words, `RMSE` will indicate an approximate amount of Euros we are on average off in our rental price predictions.

**Definition:** (`RMSE`)

Let $r_{o}$ be the true rental price for an offer $o$, $N_{o}$ be the number of offers, and $\hat{r}_{o}$ be our prediction, then:

\[
  RMSE=\sqrt{\sum_{m=1}^{N_{o}}(r_{o}-\hat{r}_{o})^2}
\]

The definition above trivially translates into the following `R` function:

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

```{r, echo=F}
TARGET_RMSE <- 50
```
The ultimate goal of the project is to provide a statistical model, solely based on the modeling set, that on the `validation` set will be able to predict apartment rental prices with `RMSE` $\leq$ `TARGET_RMSE` = `r TARGET_RMSE`. The way achieve that goal will be explained in the *"Modeling approach"* section of this document.

## Execution plan

Let us now briefly outline the main steps to be performed to reach the previously formalized project goal:

1. **Prepare the data** -- see the *"Data wrangling"* section:
    + Select, clean, and reshape relevant data; split it into training and validation sets; and etc.
2. **Analyze the dataset** -- see the *"Dataset analysis"* section:
    + Perform data exploration and visualization; summarize insights on the data.
3. **Describe the modeling approach** -- see the *"Modeling approach"* section:
    + Consider the insights of the data analysis; suggest the way obtain a prediction model.
4. **Present modeling results** -- see the  *"Results"* section:
    + Train the model(s); evaluate the model(s); analyze the results.
5. **Provide concluding remarks** -- see the *"Conclusions"* section:
    + Summarize the results; mention any approach limitations; outline possible future improvements.

# Data wrangling
<!-- A data wrangling section that explains how the data was prepared for data analysis -->
In this section we present cleaning, enriching, and restructuring the raw data taken from the `r arog_report$AROG_DATA_SET_NAME`.

This section will be organized as follows: First we explain how we cleaned the data and solved some of its inconsistencies, by enriching the data. Then we identify some structural changes done to the data. Further, we
provide a summary of the wrangled data set. In the end, we explain how we split the entire data set into the `validation` and `modeling` sub-sets[^4].

[^4]:The latter will also be split into the `training` and `testing` set for the sake of model cross-validation.

## Data cleaning & enriching
Let us note that the number of data entries in the original data set is equal to `r nrow(arog_data$selected_data)`. This data is however not ready to be worked with as it is very dirty.  It contains multiple `N/A` values; is inconsistent -- has  mismatching row values, e.g. `floor = 10` and `typeOfFlat = "roof_storey"`; and has multiple outliers in numerical/integer columns.

The rest of the section is organized as follows:  First, we explain cleaning of `N/A` values. Second, we discuss stripping of the data from the outliers. Third, we outline filtering out and correcting some of the data inconsistencies.

### Removing `N/A` values 
Consider for example the next table summarizing the number of `N/A` values per data set column:

```{r, echo=F}
sel_data_na_cnts <- count_na_entries(arog_data$selected_data)
sel_data_na_cnts%>% print(., n=Inf)
```
As one can see, about $\frac{1}{2}$ of the columns has $2.5$--$80$% `N/A`$^{s}$, whereas the other half has (almost) no `N/A`$^{s}$.

Cleaning the data from `N/A` values will be explained in the next steps:

1. We begin with the `totalRent` column as this is the value that we want to predict;
2. We proceed with the columns with the marginal ($< 1$%) of `N/A` values; 
3. We cover the remaining columns in the descending order of the number of `N/A` values.

#### The first steps

```{r, echo = F}
idx <- which(sel_data_na_cnts[,"Column name"] == "totalRent")
total_rent_na_percent <- sel_data_na_cnts[idx, "N/A percent"]
num_marginal_columns <- nrow(filter(sel_data_na_cnts, sel_data_na_cnts[,"N/A percent"] < 1))
```

The `totalRent` column contains data that we want to predict. Therefore, the rows with `totalRent` == `N/A` are useless to us and shall be removed. Unfortunately, this will reduce the data set by `r total_rent_na_percent`%. There are also `r num_marginal_columns` columns with a marginal ($0$ to $1$) number of `N/A` values. The latter can be seamlessly removed as even if all of these `N/A`$^{s}$ appear in different rows, we will remove at most `r num_marginal_columns` entries which is just `r round(num_marginal_columns/nrow(arog_data$selected_data)*100,4)`% of data.

#### The main columns
Let us consider the columns one by one. Note that, some modifications we will do to the data to remove the `N/A` values  may introduce bias. To for test that we would need a clean data set with no `N/A` values initially present and then to use such a data set for the trained model(s) validation. Due to the lack of time this will not be done in the case study.

##### Column: `electricityBasePrice` - $76.2$% `N/A` values
We will set the electricity base price for the `N/A` values to zero. The motivation is that, since the number of `N/A` values is almost $80$% and no other zero values are present:
```{r}
x <- arog_data$selected_data %>% filter(!is.na(electricityBasePrice))
sum(x$electricityBasePrice == 0)
```
it is likely that the `N/A` values were used to determine the fact that there is no electricity base price.

##### Column: `energyEfficiencyClass` - $72.3$% `N/A` values
The energy efficiency factor levels are: 
```{r}
levels(arog_data$selected_data$energyEfficiencyClass)
```
So we shall naturally set all the `N/A` energy efficiency levels to `"NO_INFORMATION"`.

##### Column: `heatingCosts` - $68.2$% `N/A` values
```{r, echo = F}
x <- arog_data$selected_data %>% filter(!is.na(heatingCosts))
num_zero_val <- sum(x$heatingCosts == 0)
```
We will set the heating costs for the `N/A` values to zero as there are already  `r num_zero_val` zero-valued heating cost entries. It is unlikely that there are non-heated accommodations in Germany so *we assume that the $0$ values, the same as `N/A`$^{s}$ mean - "unknown"*.

##### Column: `noParkSpaces` - $65.8$% `N/A` values
```{r, echo = F}
x <- arog_data$selected_data %>% filter(!is.na(noParkSpaces))
num_zero_val <- sum(x$noParkSpaces == 0)
```
We will set the number of parking places for the `N/A` values to zero as there is already `r num_zero_val` zero-valued entries. By this step we assume that, `N/A` is interpreted as *"not applicable"* or *"no are available"*.

##### Column: `interiorQual` - $38.8$% `N/A` values
The interior quality factor levels are: 
```{r}
levels(arog_data$selected_data$interiorQual)
```
So we shall introduce a new level for the `N/A` values, called `"unknown"`.

##### Column: `numberOfFloors` - $36.2$% `N/A` values
```{r, echo = F}
type_of_flat_levels <- levels(arog_data$selected_data$typeOfFlat)
list_of_nof_sum <- sapply(type_of_flat_levels, nr_floors_summary)
nof_sum_df <- as_tibble(t(list_of_nof_sum)) %>% arrange(desc(as.numeric(count)))
```
Setting the `N/A` values for the floors shall be agreed with the apartment type, if we gather some number of floors statistics for each available apartment type we get the following:

```{r, echo = F}
kable(nof_sum_df, caption = "Type of flat vs. number of floors statistics",
      col.names = c("typeOfFlat", "Count", "Average", "Standard error", "Minimum", "Maximum"),
      escape = TRUE, align=rep('c', 4)) %>%
  add_header_above(c(" " = 1, "numberOfFloors" = 5)) %>%
  kable_styling(latex_options =c("striped", "HOLD_position"))
```
From where we conclude that the data we have is very polluted. Clearly, one can not expect apartments with $99$ floors and alike. See also on the large average (all  `+/-` around $3$ floors) and the huge standard error values. If we visualize the results (filtering out `r sum(arog_data$selected_data$numberOfFloors > arog_report$MAX_NUM_FLOORS_TO_CONSIDER, na.rm = TRUE)` flats with more than `r arog_report$MAX_NUM_FLOORS_TO_CONSIDER` floors), we see that:

```{r, echo = F}
arog_data$selected_data %>%
  filter(!is.na(numberOfFloors) & !is.na(typeOfFlat) &
           (numberOfFloors <= arog_report$MAX_NUM_FLOORS_TO_CONSIDER)) %>%
  group_by(typeOfFlat) %>%
  ggplot() +
  geom_bar(aes(numberOfFloors)) +
  scale_y_log10() +
  facet_wrap(~typeOfFlat, nrow=3) +
  theme(plot.title = element_text(hjust = 0.5, face="bold")) +
  labs(x="The number of floors") +
  ggtitle("The distribution of number of floors per flat type") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
the data seems to be approximately normally distributed (except for the `apartment` type) with the mean values within $2.5$ - $4.0$ range. This makes us believe that this data is too much biased and polluted. So we will not rely on this column in our analysis.

##### Column: `condition` - $25.4$% `N/A` values
The condition factor levels are: 
```{r, echo = F}
levels(arog_data$selected_data$condition)
```
So we shall introduce a new level for the `N/A` values, called `"unknown"`.

##### Column: `yearConstructed` - $21.3$% `N/A` values
There is no good default to replace the `N/A` values here. Yet, it is a significant amount of data which we do not want to exclude. Therefore drop this column from the analysis and just use the `newlyConst` flag column.

##### Column: `floor` - $19.0$% `N/A` values
We could assign some `floor` values based on the flat types:
```{r, echo = F}
levels(arog_data$selected_data$typeOfFlat)
```
For example, we could consider assigning:

* `half_basement` -- set `floor` to be the average half-basement floor value
* `ground_floor` -- set `floor = 0`
* `raised_ground_floor` -- set `floor` to be the average raised ground floor value 

but, let us look at the floor values (filtering out `r sum(arog_data$selected_data$floor > arog_report$MAX_FLOOR_TO_CONSIDER, na.rm = TRUE)` flats with `floor` $>$ `r arog_report$MAX_FLOOR_TO_CONSIDER`), for these flat types:
```{r, echo = F, out.height = "50%", fig.align="center"}
arog_data$selected_data %>%
  filter(!is.na(floor) & !is.na(typeOfFlat) & (floor <= arog_report$MAX_FLOOR_TO_CONSIDER) &
           (typeOfFlat %in% c("half_basement", "ground_floor", "raised_ground_floor"))) %>%
  group_by(typeOfFlat) %>%
  ggplot() +
  geom_bar(aes(floor)) +
  scale_y_log10() +
  facet_wrap(~typeOfFlat, nrow=3) +
  labs(x="The flat's floor") +
  ggtitle("The distribution of floors per flat type") +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
From the data above we see that we shall not only correct the `N/A` values but set all of the floor values for the considered flat types as follows:

* `half_basement` -- set `floor = -1`
* `ground_floor` -- set `floor = 0`
* `raised_ground_floor` -- set `floor = 0`

```{r, echo = F}
num_rem_na_floors <- arog_data$selected_data %>%
  filter(is.na(floor) & !is.na(typeOfFlat) &
           !(typeOfFlat %in% c("half_basement", "ground_floor", "raised_ground_floor"))) %>%
  nrow()
```

If we do that then there will still be `r num_rem_na_floors` (`r round(num_rem_na_floors*100/nrow(arog_data$selected_data), 1)`% of data) `N/A` floor values for the flat types for which we can not give any exact value. So we will just assign those to the mean floor value in the category.

##### Column: `heatingType` - $16.4$% `N/A` values
The heating type factor levels are: 
```{r, echo = F}
levels(arog_data$selected_data$heatingType)
```
So we shall introduce a new level for the `N/A`, and `"H"` values, called `"unknown"`.

##### Column: `typeOfFlat` - $13.9$% `N/A` values
The type of flat factor levels are: 
```{r, echo = F}
levels(arog_data$selected_data$typeOfFlat)
```
So we shall introduce a new level for the `N/A` values, called `"unknown"`. Note that, we do not use the pre-defined level `"other"` here as we interpret it as known flat type which is just not on the list of available choices.

##### Column `serviceCharge` - $2.58$% `N/A` values
We will set the service charges for the `N/A` values to zero. The motivation is that, there are:
```{r}
x <- arog_data$selected_data %>% filter(!is.na(serviceCharge))
sum(x$serviceCharge == 0)
```
zero values present, so we interpret the `N/A` values as defining the fact of no additional service charges.

### Filtering outliers
In this section we only considered the numeric/integer columns of the data set. The initial set of outliers per column is obtained using: 
```{r, eval = F}
boxplot.stats(.)$out
```

However, not all of the obtained outlier values are the true outliers. It may be that some flats do stand out as examples of extraordinary property, and not due to owner input errors. This is why, for each of the column, the identified outliers are analyzed and it is then decided on how much of them is to be removed. 

#### Column: `noRooms`
The `noRooms` column has `r length(boxplot.stats(arog_data$selected_data$noRooms)$out)` outliers, see the plot:
```{r, echo = F, warning=FALSE, out.height = "35%", fig.align="center"}
arog_data$selected_data %>%
  ggplot(aes(x="", y=noRooms)) + 
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.3, outlier.shape = 1, outlier.size = 1) + 
  scale_y_log10() + 
  labs(x="All flats", y="Number of rooms") +
  ggtitle("Outliers of the number of rooms") +
  annotation_logticks() + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
We shall remove all the rows with `noRooms` outside the interval
[`r arog_report$MIN_MAX_OUTLIER_FILTERS$noRooms[1]`, `r arog_report$MIN_MAX_OUTLIER_FILTERS$noRooms[2]`].

#### Column: `noParkSpaces`
The `noParkSpaces` column has `r length(boxplot.stats(arog_data$selected_data$noParkSpaces)$out)` outliers, see the plot:
```{r, echo = F, warning=FALSE, out.height = "35%", fig.align="center"}
arog_data$selected_data %>%
  ggplot(aes(x="", y=noParkSpaces)) + 
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.3, outlier.shape = 1, outlier.size = 1) + 
  scale_y_log10() + 
  labs(x="All flats", y="Number of parking spaces") +
  ggtitle("Outliers of the number of parking spaces") +
  annotation_logticks() + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
We shall remove all the rows with `noParkSpaces` outside the interval
[`r arog_report$MIN_MAX_OUTLIER_FILTERS$noParkSpaces[1]`, `r arog_report$MIN_MAX_OUTLIER_FILTERS$noParkSpaces[2]`].

#### Column: `livingSpace`
The `livingSpace` has `r length(boxplot.stats(arog_data$selected_data$livingSpace)$out)` outliers, see the plot:
```{r, echo = F, warning=FALSE, out.height = "35%", fig.align="center"}
arog_data$selected_data %>%
  ggplot(aes(x="", y=livingSpace)) + 
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.3, outlier.shape = 1, outlier.size = 1) + 
  scale_y_log10() + 
  labs(x="All flats", y="Living space") +
  ggtitle("Outliers of the living space") +
  annotation_logticks() + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
We shall remove all the rows with `livingSpace` outside the interval
[`r arog_report$MIN_MAX_OUTLIER_FILTERS$livingSpace[1]`, `r arog_report$MIN_MAX_OUTLIER_FILTERS$livingSpace[2]`].

#### Column: `baseRent`
The `baseRent` has `r length(boxplot.stats(arog_data$selected_data$baseRent)$out)` outliers, see the plot:
```{r, echo = F, warning=FALSE, out.height = "35%", fig.align="center"}
arog_data$selected_data %>%
  ggplot(aes(x="", y=baseRent)) + 
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.3, outlier.shape = 1, outlier.size = 1) + 
  scale_y_log10() + 
  labs(x="All flats", y="Base rent") +
  ggtitle("Outliers of the base rent") +
  annotation_logticks() + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
We shall remove all the rows with `baseRent` outside the interval
[`r arog_report$MIN_MAX_OUTLIER_FILTERS$baseRent[1]`, `r arog_report$MIN_MAX_OUTLIER_FILTERS$baseRent[2]`].

#### Column: `electricityBasePrice`
The `electricityBasePrice` has `r length(boxplot.stats(arog_data$selected_data$electricityBasePrice)$out)` outliers, see the plot:
```{r, echo = F, warning=FALSE, out.height = "35%", fig.align="center"}
arog_data$selected_data %>%
  ggplot(aes(x="", y=electricityBasePrice)) + 
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.3, outlier.shape = 1, outlier.size = 1) + 
  scale_y_log10() + 
  labs(x="All flats", y="Electricity base price") +
  ggtitle("Outliers of the electricity base price") +
  annotation_logticks() + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
We shall remove all the rows with `electricityBasePrice` outside the interval
[`r arog_report$MIN_MAX_OUTLIER_FILTERS$electricityBasePrice[1]`, `r arog_report$MIN_MAX_OUTLIER_FILTERS$electricityBasePrice[2]`].

#### Column: `heatingCosts`
The `heatingCosts` has `r length(boxplot.stats(arog_data$selected_data$heatingCosts)$out)` outliers, see the plot:
```{r, echo = F, warning=FALSE, out.height = "35%", fig.align="center"}
arog_data$selected_data %>%
  ggplot(aes(x="", y=heatingCosts)) + 
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.3, outlier.shape = 1, outlier.size = 1) + 
  scale_y_log10() + 
  labs(x="All flats", y="Heating costs") +
  ggtitle("Outliers of the heating costs") +
  annotation_logticks() + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
We shall remove all the rows with `heatingCosts` outside the interval
[`r arog_report$MIN_MAX_OUTLIER_FILTERS$heatingCosts[1]`, `r arog_report$MIN_MAX_OUTLIER_FILTERS$heatingCosts[2]`].

#### Column: `serviceCharge`
The `serviceCharge` has `r length(boxplot.stats(arog_data$selected_data$serviceCharge)$out)` outliers, see the plot:
```{r, echo = F, warning=FALSE, out.height = "35%", fig.align="center"}
arog_data$selected_data %>%
  ggplot(aes(x="", y=serviceCharge)) + 
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.3, outlier.shape = 1, outlier.size = 1) + 
  scale_y_log10() + 
  labs(x="All flats", y="Service charge") +
  ggtitle("Outliers of the service charge") +
  annotation_logticks() + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
We shall remove all the rows with `serviceCharge` outside the interval
[`r arog_report$MIN_MAX_OUTLIER_FILTERS$serviceCharge[1]`, `r arog_report$MIN_MAX_OUTLIER_FILTERS$serviceCharge[2]`].

#### Column: `totalRent`
The `totalRent` has `r length(boxplot.stats(arog_data$selected_data$totalRent)$out)` outliers, see the plot:
```{r, echo = F, warning=FALSE, out.height = "35%", fig.align="center"}
arog_data$selected_data %>%
  ggplot(aes(x="", y=totalRent)) + 
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.3, outlier.shape = 1, outlier.size = 1) + 
  scale_y_log10() + 
  labs(x="All flats", y="Total rent") +
  ggtitle("Outliers of the total rent") +
  annotation_logticks() + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
We shall remove all the rows with `totalRent` outside the interval
[`r arog_report$MIN_MAX_OUTLIER_FILTERS$totalRent[1]`, `r arog_report$MIN_MAX_OUTLIER_FILTERS$totalRent[2]`].

### Fixing inconsistencies
In addition to the data alternations done above we have also done the following:

* Re-setting the number of floors:
    * `half_basement` --`floor` $= -1$
    * `ground_floor` -- `floor` $= 0$
    * `raised_ground_floor` --`floor` $= 0$
* Filter out flats:
    * Other than `"half_basement"`, `"other"`, and `"unknown"`; but with `floor` < $0$
    * With zero `totalRent` values (`r length(which(arog_data$selected_data$totalRent == 0))` entries)
* Fix wrong and convert scraping dates:
    * `"Sep18"` changes into `ymd("2018-09-22")`[^7]
    * `"May19"` changes into `ymd("2019-05-10")`
    * `"Oct19"` changes into `ymd("2019-10-08")`
* Made sure that the integer-valued ("floor", "noParkSpaces", "noRooms") columns are rounded.
    
[^7]: The `ymd(.)` function is provided by the `lubridate` package.

There may be more inconsistencies present in the data, for example we expect that:
```
totalRent = baseRent + electricityBasePrice + heatingCosts + serviceCharge
```
which may not be the case. Unfortunately due to the lack of time these were not analyzed further and thus may potentially influence the "accuracy" of the trained statistical model.

## Restructuring data

There data set at hand does not have any complex structure. However, because we want to be able to do predictions per city and avoid cities with the same names within different lands and regions we shall combine the `regio` columns into a new single one, as follows:
```{r, eval =F}
  clean_arog_data <- clean_arog_data %>% 
    unite("location", c("regio1", "regio2", "regio3"), remove=FALSE) %>%
    select(-regio1, -regio2, -regio3)
```
The resulting columns have values constructed according to the following pattern:
```
location = regio1 + "_" + regio2 + "_" + regio3
```
For example:
```{r}
arog_data$wrangled_data$location[1:5]
```

In addition we have rounded and turned into integer columns all the integer-valued columns of the data set:
```{r}
c("floor", "noParkSpaces", "noRooms")
```

## Wrangled data set
Let us now summarize the resulting clean data:

```{r, echo=F}
clean_data_na_cnts <- count_na_entries(arog_data$wrangled_data)
clean_data_na_cnts%>% print(., n=Inf)
```

```{r, echo = F}
selected_data_row_cnt <- nrow(arog_data$selected_data)
cleaned_data_row_cnt <- nrow(arog_data$wrangled_data)
na_total_rent_cnt <- round(selected_data_row_cnt/100*total_rent_na_percent)
selected_wtr <- nrow(arog_data$selected_data) - na_total_rent_cnt
```

As one can notice, the dat set size has been reduced from `r  nrow(arog_data$selected_data)` to `r cleaned_data_row_cnt`. The major reason for that is excluding the rows with the `N/A` values of the `totalRent` column. Let us recall that the number of such raws was `r total_rent_na_percent`% of the data set, e.g. `r na_total_rent_cnt` rows. It now remains to notice that `r nrow(arog_data$selected_data)` $-$ `r na_total_rent_cnt` $=$ `r selected_wtr` $>$ `r cleaned_data_row_cnt`. The remaining delta of `r selected_wtr - cleaned_data_row_cnt` rows ($\approx$ `r round( (selected_wtr - cleaned_data_row_cnt)*100/selected_data_row_cnt, 1)`% of data) is explained by cleaning the `floor`/`typeOfFlat` columns, filtering-out outliers, and etc.

The data has been cleaned but we can expect that there is some noise in the data which we have not addressed. We might get more data-quality insights when we perform data analysis in the subsequent sections.

## Splitting data

To facilitate supervised learning, the wrangled data is split into the `modeling`, `r (1-arog_report$VALIDATION_TO_MODELING_SET_RATIO)*100`% of data, and `validation`, `r arog_report$VALIDATION_TO_MODELING_SET_RATIO*100`% of data, sets. The former will be used for training statistical model(s) and the latter for the model(s) validation. Note that, for the sake of subsequent cross validation during modeling part, we further split the `modeling` set into the `training`, `r (1-arog_report$TEST_TO_TRAIN_SET_RATIO)*100`% thereof, and `testing`, `r arog_report$TEST_TO_TRAIN_SET_RATIO*100`% thereof, sets.

We split the data in the following steps:

1. The data is randomly split into to parts according to the specified ratio:
```{r, eval = F}
  test_index <- createDataPartition(y = data_set$totalRent, times = 1, 
                                    p = ratio, list = FALSE)
```
2. The `test_index` rows are the candidates for the `testing`/`validation` set rows
3. The factorized column values of the `testing`/`validation` set are considered:
```{r}
str_data <- capture.output(str(arog_data$wrangled_data))
str_replace_all(str_subset(str_data, "Factor"), "levels.*","levels ...")
```
4. The rows with the values not present in the `testing`/`modeling` set are dropped 
5. The `testing`/`modeling` set consists of rows absent in the `testing`/`validation` set

The procedure above ensures that the `testing`/`validation` set can always be evaluated on a model trained on the `testing`/`modeling` set. For more details, see the `create_arog_data` and `split_train_test_sets` functions located in the `apartment_rental_project.R` script.

The resulting set sizes are as follows:
```{r, echo = F}
total_data_size <- nrow(arog_data$wrangled_data)
train_set_size <- nrow(arog_data$training_data)
test_set_size <- nrow(arog_data$testing_data)
model_set_size <- train_set_size + test_set_size
valid_set_size <- nrow(arog_data$validation)
```
* `modeling` -- `r model_set_size` rows, `r round(model_set_size*100/total_data_size, 1)`% of data
    * `training` -- `r train_set_size` rows, `r round(train_set_size*100/model_set_size, 1)`% of `modeling` set
    * `testing` -- `r test_set_size` rows, `r round(test_set_size*100/model_set_size, 1)`% of `modeling` set
* `validation` -- `r valid_set_size` rows, `r round(valid_set_size*100/total_data_size, 1)`% of data

As expected, due to returning `testing`/`validation` set rows to the `testing`/`modeling` set for consistency, the desired set ratios are biased. The `validation` set size is almost as prescribed (`r arog_report$VALIDATION_TO_MODELING_SET_RATIO*100`% of data), but the `testing` set size is affected more significantly[^6]. Yet, we see no issue as the `testing` set is still $> 10$% of the `modeling` set, which should be enough for performing cross validation.

[^6]: The requested `testing` set size was `r arog_report$TEST_TO_TRAIN_SET_RATIO*100`% of the `modeling` set.

# Data analysis
<!-- A data analysis section that:
 1. Explains the process and techniques used, such as:
  1.1 Data exploration
  1.2 Data visualization
 2. Presents insights gained -->

In this section we shall use data visualization and other techniques to investigate the properties of the data we could use to build the prediction model. In the remainder of the section we shall address:

1. Possible `timing` effects
2. Possible `location` effects
3. Predictor's correlation
4. Principle component analysis

Eventually, we shall conclude the section with a short summary of our findings.

## Possible `timing` effects
Let us consider any possible effects introduced by the scraping date. In essence, the goal of this section is to understand whether:

1. We shall keep the data scraped on different date distinct, and have the `date` column as a predictor, or
2. We could consider the combined statistics for all the data ignoring the `date` field as a predictor

We answer this question based on the analysis of "Flat offers per `date`", "Flat counts per `location` per `date`", and "Average `totalRent` per `date`"

### Flat offers per `date`
Let us consider the number of flat offers per location per data scraping date:

```{r, echo = F, out.height = "50%", fig.align="center"}
arog_data$wrangled_data %>%
  ggplot() + geom_bar(aes(location)) + scale_y_sqrt() +
  labs(x="Various locations", y="Number of flats") +
  ggtitle("Offer counts per location per date") +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  facet_wrap(~date, nrow=3) +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
The number of flats listed by location seems to follow the same pattern on all of the three data scraping dates. 

It seems like we may ignore the `date` column and consider the joint statistics. To have more solid grounds for that, let us investigate the flat counts per `location` per `date`.

### Flat counts per `location` per `date`

The distribution of flat counts per `location` indicates that there are a lot of locations with just one flat. However, there are also a few locations with a "large" ($>100$) number of flat offers.

```{r, echo = F, out.height = "50%", fig.align="center"}
offers_per_location <- arog_data$wrangled_data %>% 
  group_by(date, location) %>% 
  summarize(cnt = n())

offers_per_location %>%
  ggplot(aes(x=cnt)) + 
  scale_x_log10() +
  labs(x="Number of flats", y="Number of locations") +
  ggtitle("Distribution of location flat counts") +
  facet_wrap(~date, nrow=3) +
  geom_histogram(binwidth=.05) +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

The median numbers of flats per location are equal for all the dates:
```{r, echo = F}
offers_per_location %>% ungroup() %>% group_by(date) %>% summarise(median = median(cnt))
```
The average numbers of flats per location for the last two scrapes are equal, and for the first one is somewhat smaller:
```{r, echo = F}
offers_per_location %>% ungroup() %>% group_by(date) %>% summarise(average = round(mean(cnt)))
```
Overall, the number of flats per location seems to be stable from one date to another:
```{r, echo = F, out.height = "35%", fig.align="center"}
offers_per_location %>% 
  ggplot(aes(x = "", y = cnt))  + 
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.3, outlier.shape = 1, outlier.size = 1) + 
  stat_summary(fun.y=log10_mean, geom="point", shape=18, size=4, color="#fe02e7") +
  scale_y_log10()  +
  facet_wrap(~date, nrow=1) +
  labs(x="All locations", y="Number of flats per location") +
  ggtitle("Flat counts per location") +
  annotation_logticks() + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```

It seems like we may ignore the `date` column and consider the joint statistics. To have more solid grounds for that, let us investigate the average `totalRent` per `date`.

### Average `totalRent` per `date`
Let us plot the `totalRent` per `date`:

```{r, echo = F, out.height = "35%", fig.align="center"}
arog_data$wrangled_data %>%
  group_by(date) %>% 
  ggplot(aes(x=date, y = totalRent, group=date)) +
  geom_boxplot(outlier.colour = "red", outlier.alpha = 0.3, outlier.shape = 1, outlier.size = 1) + 
  scale_y_log10() +
  labs(x="Scraping dates", y="Total rent values") +
  ggtitle("Total rent values per date") +
  annotation_logticks() + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
As we see the data seems to be distributed similarly, moreover the does not seem to be any trend in average `totalRent` per `date`, as computed with `geom_smooth(method="lm")`:
```{r, echo = F, warning = F, out.height = "35%", fig.align="center"}
  #Set some plotting limits, for visibility reasons
  NUM_PLOT_SAMPLES <- 5000
  MIN_Y_VAL <- 100
  MAX_Y_VAL <- 10000
  
  #Plot the total rent values per date
  arog_data$wrangled_data %>%
    group_by(date) %>% 
    sample_n(NUM_PLOT_SAMPLES) %>%
    ggplot(aes(x=date, y = totalRent)) +
    geom_jitter(width = 30, alpha=0.1) +
    geom_smooth(method="lm") + 
    scale_y_log10(limits = c(MIN_Y_VAL,MAX_Y_VAL)) + 
    labs(x="Scraping dates", y="Total rent values") +
    ggtitle("Total rent values per date") +
    theme(plot.title = element_text(hjust = 0.5, face="bold")) +
    theme_bw()
```
There does not seem to be any global[^9] `timing` effect in `totalRent` related to the scraping date. Please note that in the plot above, for better visualization, we took a `r NUM_PLOT_SAMPLES` sub-sample of the data and limited the $y$ axis range by [`r MIN_Y_VAL`, `r MAX_Y_VAL`]. The latter did not influence the `"lm"` computed trend.

[^9]: There may be some location specific timing effects, but we do not consider them due to the lack of time.

**Conclusion:**  We can ignore the `date` column and consider the joint statistics for all the dates.

## Possible `location` effects
Let us consider any possible effects introduced by the location. In essence, the goal of this section is to understand whether:

1. The `totalRent` values are `location` dependent
    + Suggests using the `location` column as a predictor
2. The offered flat counts per `location`:
    1. Have influence on the number of `totalRent` value outliers
        + Suggests using Regularization in statistical modeling
    2. Are correlated with the `totalRent` values
        + Facilitates using the `location` column as a predictor

We answer this question based on the analysis of "Average `totalRent` per `location`", "Min/Max `totalRent` per `location` flat count", and "Average `totalRent` per `location` flat count".

### Average `totalRent` per `location`
Let us consider the variability of the `totalRent`, for several randomly chosen locations, with $> 100$ offers:

```{r, echo = F, out.height = "35%", fig.align="center"}
  set.seed(6)
  locations <- offers_per_location %>% 
    filter(cnt > 100) %>% 
    pull(location) %>% unique()
  sampled_locations <- sample(locations, 10)

  #Plot the sparse matrix
  left_join(arog_data$wrangled_data, offers_per_location, by="location") %>% 
    filter(location %in% sampled_locations) %>% 
    mutate(location = str_replace(location, ".*[_,/]","")) %>%
    ggplot(aes(x=location, y=totalRent, fill=cnt)) +
    scale_y_log10() + 
    geom_boxplot(outlier.colour = "red", outlier.alpha = 0.3,
                 outlier.shape = 1, outlier.size = 1) +
    labs(x="Selected locations", y="Total rent values") +
    ggtitle("Total rent variability between locations") +
    theme(plot.title = element_text(hjust = 0.5, face="bold"),
          axis.text.x = element_text(angle = 12))
```

**Conclusion:** Locations differ quite significantly in their `totalRent` -- this is a `location` effect.

### Average `totalRent` per `location` flat count

Let us plot the `totalRent` per location flat count, ordered by the count. Also to show the trend we will use `ggplot` with `geom_smooth(method = 'gam')`.

```{r, echo = F, warning = F, out.height = "35%", fig.align="center"}
  offers_per_location <- arog_data$wrangled_data %>%
  group_by(location) %>% summarise(cnt = n())
  
  #Set some plotting limits, for visibility reasons
  NUM_PLOT_SAMPLES <- 15000
  MIN_Y_VAL <- 100
  MAX_Y_VAL <- 10000
  
  left_join(arog_data$wrangled_data, offers_per_location, by="location") %>% 
    arrange(cnt) %>%
    sample_n(NUM_PLOT_SAMPLES) %>%
    ggplot(aes(x=cnt, y=totalRent)) +
    geom_jitter(width=0.05, alpha=0.1) +
    scale_x_log10() +
    scale_y_log10(limits = c(MIN_Y_VAL, MAX_Y_VAL)) +
    geom_smooth(method = 'gam') +
    labs(x="Number of flats per location", y="Flat total rent") +
    ggtitle("The total rent vs. flats per location") +
    theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
Please note that in the plot above, for better visualization, we took a `r NUM_PLOT_SAMPLES` sub-sample of the data and limited the $y$ axis range by [`r MIN_Y_VAL`, `r MAX_Y_VAL`]. The latter did not influence the `"gam"` computed trend.

**Conclusion:** There is a trend in that the places with more flat offers have on-average lower `totalRent` values - this is a `number of offers` effect.

### Min/Max `totalRent` per `location` flat count
```{r, echo = F}
NUM_TOP_BOTTOM_ROWS <- 300
```
Let us the maximum and minimum `totalRent` values relative to the flat count per `location`. In other words, each `location` has a number of flats offered. If we now combine the flat offers per location into a single totally ordered *flat-offers range*, we can plot how many locations there are with that many orders. Or we can plot the `totalRent` values offered in locations with the same number of flats. The latter will help us to see if the extreme `totalRent` values, like `r NUM_TOP_BOTTOM_ROWS` `top` and `r NUM_TOP_BOTTOM_ROWS` `bottom` priced flats are more likely to be encountered in locations with lower or larger number of flat offers.
```{r, echo = F}
  #Get the flats with the top 100 totalRent values
  top_total_rent <- arog_data$wrangled_data %>%
    left_join(., offers_per_location, by="location") %>%
    arrange(desc(totalRent)) %>% 
    slice(1:NUM_TOP_BOTTOM_ROWS) %>%
    select(cnt, totalRent, location) %>%
    mutate(Type = "Top rates")
  
  #Get the flats with the bottom 100 totalRent values
  bottom_total_rent <- arog_data$wrangled_data %>%
    left_join(., offers_per_location, by="location") %>%
    arrange(totalRent) %>% 
    slice(1:NUM_TOP_BOTTOM_ROWS) %>%
    select(cnt, totalRent, location) %>% 
    mutate(Type = "Bottom rates")
```
If we indicate `top` and `bottom` priced flats on the plot then we can observe:
```{r, echo = F, warning = F, out.height = "35%", fig.align="center"}
  left_join(arog_data$wrangled_data, offers_per_location, by="location") %>% 
    arrange(cnt) %>%
    sample_n(NUM_PLOT_SAMPLES) %>%
    ggplot(aes(x=cnt, y=totalRent)) +
    geom_jitter(width=0.05, alpha=0.25) +
    scale_x_log10() +
    scale_y_log10() +
    geom_point(data = top_total_rent, aes(cnt, totalRent), color="blue") +
    geom_point(data = bottom_total_rent, aes(cnt, totalRent), color="red") +
    labs(x="Number of flats per location", y="Flat total rent") +
    ggtitle("The top/bottom total rent vs. flats per location") +
    theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
that it seems that[12^] the `top`/`bottom` priced rentals are, almost uniformly, spread all over the *flat-offers range*. 
[^12]: Please note that in this plot, for better visualization, we took a `r NUM_PLOT_SAMPLES` sub-sample of the data.
```{r, echo = F}
  #Given a list of locations this function computes
  #the total number of flat offers
  cnt_in_all_locs <- function(locations) {
    locations <- unique(locations)
    sum <- 0
    for(loc in locations) {
      val <- offers_per_location %>% filter(location == loc) %>% pull(cnt)
      sum <- sum + val
    }
    return(sum)
  }
  
  # This function plots the number of outliers versus
  # total number of offers on the flat offers per
  # location range.
  plot_outlier_set <- function(cnt_to_outliers_cnt, type) {
    #Plot the data
    title <- paste(paste("The",type),"rent counts vs. flats per location")
    cnt_to_outliers_cnt %>%
      ggplot(aes(x=cnt)) + 
      scale_x_log10() +
      scale_y_log10() +
      geom_area(aes(y=total_cnt, fill=I("green")), 
                color= "black", linetype="solid", alpha=0.3) +
      geom_area(aes(y=outliers_cnt, fill=I("red")),
                color= "black", linetype="solid", alpha=0.3) +
      labs(x="Flats per location", y="Number of totalRent values") +
      ggtitle(title) +
      theme(plot.title = element_text(hjust = 0.5, face="bold")) +
      scale_fill_discrete(name = "Per location", labels = c("Total #offers", "#Offer outliers"))
  }
```
To get a better view on data, considering `top` and total `totalRent` counts versus the *flat-offers range*:
```{r, echo = F, out.height = "35%", fig.align="center"}
  top_counts <- top_total_rent%>%
    group_by(cnt) %>%
    summarise(outliers_cnt = n(), 
              total_cnt = cnt_in_all_locs(location))
  plot_outlier_set(top_counts, "Top")
```
Considering `bottom` and total `totalRent` counts versus the *flat-offers range*:
```{r, echo = F, out.height = "35%", fig.align="center"}
  bot_counts <- bottom_total_rent%>%
    group_by(cnt) %>%
    summarise(outliers_cnt = n(), 
              total_cnt = cnt_in_all_locs(location))
  plot_outlier_set(bot_counts, "Bottom")
```

The total count is computed as the sum of offers in the locations with the same number of offers. Effectively it is equal to `"number of offers in location"` $\times$ `"number of locations with this number of offers"`. This explains almost line-like behavior of the `"Total #offers"` plots. The irregularities are caused by having more than one location with the given number of flat offers (mind the `log10` scale of the $x$ and $y$ axis).

The number of `bottom` outliers is $\approx$ uniform across the *flat-offers range*. The number of `top` outliers looks somewhat biased to be larger for locations with more offers. Let us plot the outliers to total ratio:

```{r, echo = F, out.height = "35%", fig.align="center"}
  ggplot() + 
    geom_point(data = bot_counts, 
              aes(x=cnt, y=outliers_cnt/total_cnt, color="blue")) +
    geom_point(data = top_counts,
              aes(x=cnt, y=outliers_cnt/total_cnt, color= "red")) + 
    scale_x_log10() + scale_y_sqrt() +
    scale_color_discrete(name = "Legend:", labels = c("% Bottom outliers", "% Top outliers")) +
    labs(x="Flats per location", y="Percent of outliers") +
    ggtitle("Percent of outliers vs. flats per location") +
    theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
The plot above has, even though contains quite some "noise" for $50$-$200$ flats per location, indicates the common trend of having proportionally less outliers in locations with more rental offers.

**Conclusion 1:** It is less likely to have a cheap rentals in locations with large number of offers.
**Conclusion 2:** It is less likely to have an expensive rentals in locations with a larger number of offers.

## Predictor's correlation
```{r, echo = F}
num_predictors <- ncol(arog_data$wrangled_data) - 1
```
In this case-study we have selected `r num_predictors` potential predictors:
```{r, echo = F}
setdiff(names(arog_data$wrangled_data), "totalRent")
```
As we have discussed in the `timing` effects  section the `date` column seems to be eligible for exclusion. However, to give this a more formal ground and also to reduce the number of predictors even further let us consider the correlation matrix of the training data columns:

```{r}
  #First create the predictors matrix, from the wrangled data
  predictors_mtx <- arog_data$wrangled_data %>% 
    select(-totalRent) %>% 
    data.matrix(.)
  
  #Next compute the correlations matrix and round off the values
  corr_mtx <- predictors_mtx %>%
    cor(.) %>% round(., 3)
```
When visualized, we see that there is quite a few predictors with strong correlation:

```{r, echo = F, fig.align="center"}
  #Plot the correlation values as an image
  idx_seq <- 1:num_predictors
  image(idx_seq, idx_seq,
        corr_mtx[num_predictors:1,],
        useRaster=TRUE,
        axes=FALSE,
        xlab="", ylab="",
        xaxs="r", yaxs="r") %>%
      abline(h = idx_seq + 0.5, v = idx_seq + 0.5, col = "grey") %>%
      title(main = "Predictor columns correlation matrix",
            xlab="Predictor idx", ylab="Predictor idx", line = 1)
      axis(2, at=idx_seq, labels=as.character(num_predictors:1), lwd=0, pos=0.5)
      axis(3, at=idx_seq, labels=as.character(idx_seq), lwd=0, pos=20.15)

  #Put the correlation values into the image for better plot readability
  for(i in 1:length(idx_seq)){ 
    for(j in 1:length(idx_seq)){
        text(x=length(idx_seq) +1 - i, y=j, as.character(corr_mtx[j,i]), cex = 0.45, col = "black", font=2)
    }
  }
```
For instance, `heatingType` (idx: $2$) is strongly correlated with `garden` (idx: $5$), `typeOfFlat` (idx: $9$), `condition` (idx: $12$), `interiorQual` (idx: $14$), `electricityBasePrice` (idx: $18$), and `date` (idx: $21$).

Having highly correlated columns implies that we could use less predictors to build a statistical model without loosing much accuracy.

## Principle Component Analysis

As motivated by the correlation analysis above, here we will use the Principle Component Analysis (PCA) to see if we have a distance preserving transformation of our data that gives us a new feature-space basis in which most of the data variability is explained by fewer predictors:
```{r}
  #Run the PCA analysis
  pca_result <- prcomp(predictors_mtx)
  #Report the summary
  summary(pca_result)
```
As one can see from the PCA summary above, according to Cumulative Proportion of Variance, it shall suffice to use the first two principle components (`PC1`, and `PC2`) to explain $\approx 99.3$% of the data.

## Data analysis summary
In the *"Data analysis"* section we have analyzed our data with respect to possible `timing`, and `location` effects. In addition, to reduce the number of predictors we've performed predictor's correlation and principle component analysis. The findings of this section can be summarized as follows.

### Data effects:

In case we are to build our own statistical model, as opposed using one of the already available via the `cared` package of `R`, we should:

1. The `timing` effect is not confirmed:
    + We can ignore the `date` column and consider the joint statistics for all the dates.
2. The `location` effects are confirmed:
    + The strong correlation of `location` with `totalRent` has to be taken into account[^11]

[^11]: This includes the discovered correlation from the `flat counts` per location as the flat offers per location do not very much with time.

### Dimension reduction:

Due to a high correlation of multiple predictors we can significantly reduce the dimensionality of the feature space. It suffices to use the first two/six principle components to explain $\approx 99.3$/$99.99$% of the data.

# Modeling approach
<!-- A modeling approach section that explains the modeling approach -->

To begin, let us recall the project's goal and one important data observation gained:

* **Goal**: Build a prediction model for the `totalRent` values with an `RMSE` score $\leq 50$;
* **Data**: About $99.3$% data variability is in the first two principle components;

Let us make a scatter plot of `PC1` vs `PC2` indicating the corresponding `totalRent` values with color:

```{r, echo = F, out.height = "35%", fig.align="center"}
  # This function linearly shifts the PC vector to the 
  # positive values for the sake of log scale plotting
  log_scale_shift <- function(pc) {
      min_pc <- min(pc)
    if(min_pc <= 0) {
      pc <- pc + abs(min_pc) + 1
    }
    return(pc)
  }
    
  #Set some plotting limits, for visibility reasons
  NUM_PLOT_SAMPLES <- 5000
  MIN_Y_VAL <- 70
  MAX_Y_VAL <- 10000

  #Plot the PC values with the color defined by the total rent value
  set.seed(1)
  data.frame(PC1 = log_scale_shift(pca_result$x[,1]),
             PC2 = log_scale_shift(pca_result$x[,2]),
             totalRent = arog_data$wrangled_data$totalRent) %>%
    sample_n(NUM_PLOT_SAMPLES) %>% 
    ggplot(aes(PC1, PC2, color=log10(totalRent))) +
    scale_y_log10(limits=c(MIN_Y_VAL, MAX_Y_VAL)) +
    geom_point() + scale_colour_gradientn(colours = rainbow(100)) +
    labs(x="PC1 values", y="PC2 values") +
    ggtitle("totalRent values clasterization vs. PC1 & PC2") +
    theme(plot.title = element_text(hjust = 0.5, face="bold"))
```
Here, for better visualization, we:

* Take a `r NUM_PLOT_SAMPLES` sub-sample of data;
* Limit the `PC2` range by [`r MIN_Y_VAL`,`r MAX_Y_VAL`];
* Linearly shift both `PC1` and `PC2` to the positive half-space;
* Use log10 scale for both `PC1` and `PC2`;

From the plot above, there is a clear clasterization of the `totalRent` values. However, its structure is intricate and is not likely to be captured, with desired accuracy, with a simple (linear) regression models. Thus, instead of creating a new Bayesian model, we shall attempt using the existing ones that can be trained via the `caret` package of `R`. Given that the `totalRent` column is numeric, we will consider the following models[^13]:

* `"lm"` -- Linear Regression
* `"glm"` -- Generalized Linear Model
* `"knn"` -- k-Nearest Neighbor Classification
* `"Rborist"` -- Random Forest[^14]
* `"svmLinear"` -- Support Vector Machines with Linear Kernel
* `"gamLoess"` -- Generalized Additive Model using LOESS

[^13]: For more information see also the [list of caret's Available Models](https://topepo.github.io/caret/available-models.html).
[^14]: This is a high-performance implementation of a random forest model `"rf"`

The algorithms above will be run with the default parameters, unless tuning grid parameters are required. Therefore we will consider the entire `modeling` set, as any cross-validation will be done within the `train(.)` method of the `caret` package. Last but not least, we will only use the first two principle components of the available predictors data. Any models that will fail to be trained within (`GLOBAL_METHOD_TIME_OUT_SECONDS = ` `r arog_report$GLOBAL_METHOD_TIME_OUT_SECONDS`) seconds will be discarded from the further use. Any additional model training parameters, if required, will be presented in the *"Results"* section of this report.

## Modeling code snippets
To further clarify our approach, below we provide some of the code used for model training and evaluation. First, we present the main utility functions. Next, we show their use in a single *model-train-evaluate* sequence.

As for PCA analysis, we convert our data into a numerical-data matrix using the next function:
```{r, eval = FALSE}
#--------------------------------------------------------------------
# This function prepares predictors data matrix from a given data set.
#    data_set -- the data set to prepare the  data matrix from
# The performed steps are:
#    1. Remove the totalRent column
#    2. Converting to numeric (data) matrix
# The resulting matrix is returned "as-is"
#--------------------------------------------------------------------
prepare_data_matrix <- function(data_set) {
  #Compute the full matrix from the data set
  data_mtx <- data_set %>% select(-totalRent) %>% data.matrix(.)
  
  #Return the predictors matrix
  return(data_mtx)
}
```

To get the selected PC predictors matrix from the numerical-data matrix we use:
```{r, eval = FALSE}
#--------------------------------------------------------------------
# This function takes:
#    pca_result - the PCA analysis results
#    data_mtx - the data matrix
#    num_pc - the number of PC to consider, defaults to NUM_PC_TO_CONSIDER
# and transforms the data_mtx into the PC matrix by:
#    1. Zero-centering the data_mtx columns
#    2. Applying pca_result$rotation matrix
#    3. Selecting num_pc first columns
# The resulting matrix is returned "as-is"
#--------------------------------------------------------------------
prepare_pc_predictors <- function(pca_result, data_mtx, num_pc = NUM_PC_TO_CONSIDER) {
  #Zero-center the columns
  cent_pred_mtx <- sweep(data_mtx, 2, colMeans(data_mtx))
  
  #Rotate to move to the new basis
  rot_pred_mtx <- cent_pred_mtx %*% pca_result$rotation
  
  #Only return the required principle component columns
  return(rot_pred_mtx[,1:num_pc])
}
```

The model training is be with the next function, mind the time-out handling:
```{r, eval = FALSE}
#--------------------------------------------------------------------
# This function allows to train a model specified by the method
#    data_mtx - the numeric-valued predictors data matrix
#    exp_res - the expected results vector for the predictors
#    method - the method to be used
# The training will be done with a time-out defined by the 
#   GLOBAL_METHOD_TIME_OUT_SECONDS
# The result is the list with the following elements:
#    method - the method used
#    start_time - the time the training started
#    success - the success indicating flag
#    end_time - the time the training finished, if success == TRUE
#    fit_model - the fit model, if success == TRUE
#--------------------------------------------------------------------
train_model <- function(data_mtx, exp_res, method, ...) {
  #Remove the fit model global if it exists
  ifrm(fit_model)
  
  #Initialize new empty training results list
  train_res <- list(method = method)
  
  #Train the model, with a time-out
  tryCatch({
    train_res <- withTimeout({
      #Record the start time
      train_res <- append(train_res, list(start_time = Sys.time()))
      
      #Fit the model from data
      fit_model <- train(data_mtx, exp_res, method = method, ...)
      
      #Record the end time and the result
      train_res <- append(train_res, list(fit_model = fit_model))
    }, timeout = GLOBAL_METHOD_TIME_OUT_SECONDS)
  }, TimeoutException = function(ex) {
    message("Timeout (", GLOBAL_METHOD_TIME_OUT_SECONDS, 
            " sec.) while training the '", method, "' model, skipping!")
  })

  #Mark the success flag
  train_res <- append(train_res, 
                      list(end_time = Sys.time(), 
                           success = !is.null(train_res$fit_model)))
  
  #Remove the fit model global if it exists
  ifrm(fit_model)
  
  #Return the result
  return(train_res)
}
```

The model validation is done by the next function:
```{r, eval = FALSE}
#--------------------------------------------------------------------
# The model evaluation matrix takes the:
#     mdl_res - the modeling results with the fit model to make predictions
#     pc_mtx - the validation set selected pc predictors matrix
#     exp_res - the actual validation set values
# Once the model predicts the values are the RMSE score is computed.
# The result of the function is the list with the following elements:
#    mdl_res - the modeling results
#    pc_mtx  - the selected pc predictors matrix
#    exp_res - the expected (true) values to compare with
#    act_res - the actually predicted values
#    rmse    - the RMSE score between exp_res and act_res
#--------------------------------------------------------------------
evaluate_model <- function(mdl_res, pc_mtx, exp_res) {
  if(mdl_res$success) {
    #Predict the raw data based in the fit model and predictor values
    act_res <- predict(mdl_res$fit_model, pc_mtx, type = "raw")
    
    #Compute the RMSE score
    rmse <- RMSE(act_res, exp_res)
  } else {
    #Training failed so return the NA results
    act_res <- NA 
    rmse    <- NA
  }
  
  #Create the resulting list and return
  return(list(mdl_res = mdl_res, pc_mtx  = pc_mtx,
              exp_res = exp_res, act_res = act_res, 
              rmse    = rmse))
```

With the functions above t complete *model-train-evaluate* sequence for a `KNN` look as follows:

```{r, eval = FALSE}
#Get the modeling and validation data
model_set <- arog_data$modeling_data
valid_set <- arog_data$validation_data

#Compute the data matrixes for the data sets
model_mtx <- prepare_data_matrix(model_set)
valid_mtx <- prepare_data_matrix(valid_set)

#Perform the PCA analysis on the modeling matrix
pca_result <- prcomp(model_mtx)

#Prepare the predictors for the modeling and validation sets
model_pc_mtx <- prepare_pc_predictors(pca_result, model_mtx)
valid_pc_mtx <- prepare_pc_predictors(pca_result, valid_mtx)

#Train and validate the KNN model
knn_train_res <- train_model(model_pc_mtx, model_set$totalRent, "knn",
                             tuneGrid = data.frame(k = seq(13, 18, 1)))
knn_mdl_res <- evaluate_model(knn_train_res, valid_pc_mtx, valid_set$totalRent)
knn_mdl_res$rmse
```

For more details, see the supplied `apartment_rental_project.R` modeling script.

# Results
<!-- A results section that:
 1. Presents the modeling results 
 2. Discusses the model performance -->

## Used hardware
The experiments were run on the following hardware at hand:
```
    Model Name: MacBook Pro
    Model Identifier: MacBookPro11,4
    Processor Name: Intel Core i7
    Processor Speed: 2,2 GHz
    Number of Processors: 1
    Total Number of Cores: 4
    L2 Cache (per Core): 256 KB
    L3 Cache: 6 MB
    Memory: 16 GB
    Boot ROM Version: 194.0.0.0.0
    SMC Version (system): 2.29f24
```

## Modeling results

### Linear Regression -- `lm`

### Generalized Linear Model -- `glm`

### K-Nearest Neighbors -- `knn`

### Random Forest -- `Rborist`

### Support Vector Machines -- `svmLinear`

### Generalized Additive Model -- `gamLoess`

# Results summary

# Conclusions
<!-- A conclusion section that:
 1. Gives a brief summary of the report
 2. Explains its limitations
 3. Suggests future work -->
 
## Future work

Check for introducing any bias by data wrangling.
Use model ensembles.
Use more principle components.
Factor the totalRent and use e.g. `knn3`.

# Appendix A: The complete list of data set columns

Hereby we present the complete list of columns from the original `r arog_report$AROG_DATA_SET_NAME`:

```{r, echo = F}
names(arog_data$raw_data)
```

# Appendix B: Data set column descriptions

Here is the list of the initially considered data set columns with the descriptions thereof:

1. `hasKitchen` -- has a kitchen
2. `balcony` -- does the object have a balcony
3. `cellar` -- has a cellar
4. `lift` -- is elevator available
5. `floor` -- which floor is the flat on
6. `garden` -- has a garden
7. `noParkSpaces` -- number of parking spaces
8. `livingSpace` -- living space in sqm
9. `condition` -- condition of the flat
10. `interiorQual` -- interior quality

11. `regio1` --  Bundesland
12. `regio2` - District or Kreis, same as geo krs
13. `regio3` -- City/town

14. `noRooms` -- number of rooms
15. `numberOfFloors` -- number of floors in the building
16. `typeOfFlat` -- type of flat
17. `yearConstructed` -- construction year
18. `newlyConst` -- is the building newly constructed
19. `heatingType` -- Type of heating
20. `energyEfficiencyClass` -- energy efficiency class

21. `heatingCosts` -- monthly heating costs in â‚¬
22. `serviceCharge` --  auxiliary costs such as electricity or Internet in â‚¬
23. `electricityBasePrice` -- monthly base price for electricity in â‚¬
24. `baseRent` -- base rent without electricity and heating
25. `totalRent` -- total rent (usually a sum of base rent, service charge and heating cost)

26. `date` -- time of scraping
