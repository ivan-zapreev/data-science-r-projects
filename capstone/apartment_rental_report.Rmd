---
title: "Apartment Rental Prediction System"
author: "Dr. Ivan S. Zapreev"
date: "`r format(Sys.Date())`"
output: pdf_document
number_sections: true
toc: true
---

```{r setup, include=FALSE}
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(graphics)) install.packages("graphics", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")

knitr::opts_chunk$set(echo = TRUE)

library(tidyr)
library(tidyverse)
library(lubridate)
library(graphics)
library(kableExtra)
library(dslabs)
library(dplyr)

options(digits=7)

MOVIELENS_DATA_FILE_NAME <- "arog_data.rda"
MOVIELENS_REPORT_FILE_NAME <- "arog_report.rda"

#Load the files
load(MOVIELENS_DATA_FILE_NAME)
load(MOVIELENS_REPORT_FILE_NAME)

#--------------------------------------------------------------------
# This function counts the number of N/A entries in the columns of 
# the provided data set, it returns a tibble with the columns:
#    name - storing the column name of the data set
#    count - storing the number of N/A entries
#    percent - storing the percent of N/A entries
#--------------------------------------------------------------------
count_na_entries <- function(data_set) {
  col_names <- names(data_set)
  na_counts <- as.vector(sapply(col_names, function(name){
    return(sum(is.na(data_set[,name])))
  }))
  num_rows <- nrow(data_set)
  percents <- round(na_counts*100/num_rows, 2)
  result <- tibble(name = col_names, 
                count = na_counts,
                percent = percents) %>%
    arrange(desc(count))
  names(result) <- c("Column name", "N/A count", "N/A percent")
  return(result)
}

#--------------------------------------------------------------------
# This function is used to make the number of floors summary for an
# apartment types. It accepts an apartment type and a data set, which
# defaults to arog_data$selected_data, and then returns a tibble, with 
# the following data for the flats of the app_type type:
#    app_type - apartment type
#    count - the number of non N/A numberOfFloors
#    avg - average of numberOfFloors values
#    se - standard error of numberOfFloors values
#    min - min numberOfFloors value
#    max - max numberOfFloors value
#--------------------------------------------------------------------
nr_floors_summary <- function(app_type, data_set = arog_data$selected_data) {
  df <- data_set %>%
    filter(!is.na(numberOfFloors) & (typeOfFlat == app_type)) %>% 
    summarise(app_type = app_type,
              count = n(),
              avg = round(mean(numberOfFloors), 1), 
              se = round(sd(numberOfFloors), 1),
              min = min(numberOfFloors),
              max = max(numberOfFloors))
  return(as_tibble(df))
}
```

# Introduction
<!-- An introduction/overview/executive summary section that:
 1. Describes the dataset
 2. Summarizes the goal of the project 
 3. Outlines the key steps that were performed; -->


## Dataset overview

As stated on the [webpage](`r arog_report$AROG_DATA_SET_SITE_URL`) of the `r arog_report$AROG_DATA_SET_NAME`, it contains `198,379` rental offers scraped from the Germany's biggest real estate online platform ß [ImmobilienScout24](https://www.immobilienscout24.de/). 

The data set consists of a single CSV file: *`r arog_report$AROG_CSV_FILE_NAME`* which only contains offers for rental properties. The data features important rental property attributes, such as the living area size, the rent (both base rent as well as total rent), the location, type of energy, and etc. The `date` column present in the data set defines the time of scraping, which was done on three distinct dates: *2018-09-22*, *2019-05-10* and *2019-10-08*.

The complete list of data set columns is extensive[^1] and thus in this study we will use the following subset:
```{r, echo=F}
names(arog_data$selected_data)
```
This sub-selection reduces the number of considered data set columns[^2] from $48$ to $26$ and is motivated by the personal preferences of the report's author and has no scientifically proven motivation. On the contrary, this column selection shall be seen as a part of problem statement. In other words, the task is to build an accurate[^3] rental price prediction model based on the predictors from this set of columns.

The additional data preparation steps will be described in the *"Data wrangling"* section of this document.

[^1]: Please consider reading *"Appendix A"* for the complete list of the data set columns.
[^2]: Please consider reading *"Appendix B"* for the column descriptions.
[^3]: Please consider reading the *"Project goal"* section for an exact goal formulation.

## Project goal



## Execution plan

Let us now briefly outline the main steps to be performed to reach the previously formalized project goal:

1. **Prepare the data** -- see the *"Data wrangling"* section:
    + Select, clean, and reshape relevant data; split it into training and validation sets; and etc.
2. **Analyze the dataset** -- see the *"Dataset analysis"* section:
    + Perform data exploration and visualization; summarize insights on the data.
3. **Describe the modeling approach** -- see the *"Modeling approach"* section:
    + Consider the insights of the data analysis; suggest the way for building the prediction model.
4. **Present modeling results** -- see the  *"Results"* section:
    + Train the model on the `modeling` set; analyze the training results; evaluate on the `validation` set.
5. **Provide concluding remarks** -- see the *"Conclusions"* section:
    + Summarize the results; mention any approach limitations; outline possible future improvements.

# Data wrangling
<!-- A data wrangling section that explains how the data was prepared for data analysis -->
In this section we present cleaning, enriching, and restructuring the raw data taken from the `r arog_report$AROG_DATA_SET_NAME`.

This section will be organized as follows: First we explain how we cleaned the data and solved some of its inconsistencies, by enriching the data. Then we identify some structural changes done to the data. Further, we
provide a summary of the wrangled data set. In the end, we explain how we split the entire data set into the `validation` and `modeling` sub-sets[^4].

[^4]:The latter will also be split into the `training` and `testing` set for the sake of model cross-validation.

## Data cleaning & enriching
Let us note that the number of data entries in the original data set is equal to `r nrow(arog_data$selected_data)`. This data is however not ready to be worked with as, for instance, it contains multiple `N/A` values and there are also other inconsistencies present.

Consider the next table summarizing the number of `N/A` values per data set column:

```{r, echo=F}
sel_data_na_cnts <- count_na_entries(arog_data$selected_data)
sel_data_na_cnts%>% print(., n=Inf)
```
As one can see, about $\frac{1}{2}$ of the columns has $10$--$80$% `N/A`$^{s}$, whereas the other half has (almost) no `N/A`$^{s}$.

The data cleaning and enriching will be explained in the next steps:

1. We begin with the `totalRent` column as this is the value that we want to predict;
2. We proceed with the columns with the marginal ($< 1$%) of `N/A` values; 
3. We cover the remaining columns in the descending order of the number of `N/A` values.
4. We consider and sole some other data inconsistencies.

### The first steps

```{r, echo = F}
idx <- which(sel_data_na_cnts[,"Column name"] == "totalRent")
total_rent_na_percent <- sel_data_na_cnts[idx, "N/A percent"]
num_marginal_columns <- nrow(filter(sel_data_na_cnts, sel_data_na_cnts[,"N/A percent"] < 1))
```

The `totalRent` column contains data that we want to predict. Therefore, the rows with `totalRent` == `N/A` are useless to us and shall be removed. Unfortunately, this will reduce the data set by `r total_rent_na_percent`%. There are also `r num_marginal_columns` columns with a marginal ($0$ to $1$) number of `N/A` values. The latter can be seamlessly removed as even if all of these `N/A`$^{s}$ appear in different rows, we will remove at most `r num_marginal_columns` entries which is just `r round(num_marginal_columns/nrow(arog_data$selected_data)*100,4)`% of data.

### The main columns
Let us consider the columns one by one. Note that, some modifications we will do to the data to remove the `N/A` values  may introduce bias. To for test that we would need a clean data set with no `N/A` values initially present and then to use such a data set for the trained model(s) validation. Due to the lack of time this will not be done in the case study.

#### Column: `electricityBasePrice` - $76.2$% `N/A` values
We will set the electricity base price for the `N/A` values to zero. The motivation is that, since the number of `N/A` values is almost $80$% and no other zero values are present:
```{r}
x <- arog_data$selected_data %>% filter(!is.na(electricityBasePrice))
sum(x$electricityBasePrice == 0)
```
it is likely that the `N/A` values were used to determine the fact that there is no electricity base price.

#### Column: `energyEfficiencyClass` - $72.3$% `N/A` values
The energy efficiency factor levels are: 
```{r}
levels(arog_data$selected_data$energyEfficiencyClass)
```
So we shall naturally set all the `N/A` and `""` energy efficiency levels to `"NO_INFORMATION"`.

#### Column: `heatingCosts` - $68.2$% `N/A` values
```{r, echo = F}
x <- arog_data$selected_data %>% filter(!is.na(heatingCosts))
num_zero_val <- sum(x$heatingCosts == 0)
```
We will set the heating costs for the `N/A` values to zero as there are already  `r num_zero_val` zero-valued heating cost entries. It is unlikely that there are non-heated accommodations in Germany so *we assume that the $0$ values, the same as `N/A`$^{s}$ mean - "unknown"*.

#### Column: `noParkSpaces` - $65.8$% `N/A` values
```{r, echo = F}
x <- arog_data$selected_data %>% filter(!is.na(noParkSpaces))
num_zero_val <- sum(x$noParkSpaces == 0)
```
We will set the number of parking places for the `N/A` values to zero as there is already `r num_zero_val` zero-valued entries. By this step we assume that, `N/A` is interpreted as *"not applicable"* or *"no are available"*.

#### Column: `interiorQual` - $38.8$% `N/A` values
The interior quality factor levels are: 
```{r}
levels(arog_data$selected_data$interiorQual)
```
So we shall introduce a new level for the `N/A` and `""` values, called `"unknown"`.

#### Column: `numberOfFloors` - $36.2$% `N/A` values
```{r, echo = F}
type_of_flat_levels <- levels(arog_data$selected_data$typeOfFlat)
list_of_nof_sum <- sapply(type_of_flat_levels, nr_floors_summary)
nof_sum_df <- as_tibble(t(list_of_nof_sum)) %>% arrange(desc(as.numeric(count)))
nof_sum_df[which(nof_sum_df$app_type==""), ]$app_type <- '""'
```
Setting the `N/A` values for the floors shall be agreed with the apartment type, if we gather some number of floors statistics for each available apartment type we get the following:

```{r, echo = F}
kable(nof_sum_df, caption = "Type of flat vs. number of floors statistics",
      col.names = c("typeOfFlat", "Count", "Average", "Standard error", "Minimum", "Maximum"),
      escape = TRUE, align=rep('c', 4)) %>%
  add_header_above(c(" " = 1, "numberOfFloors" = 5)) %>%
  kable_styling(latex_options =c("striped", "HOLD_position"))
```
From where we conclude that the data we have is very polluted. Clearly, one can not expect apartments with $99$ floors and alike. See also on the large average (all  `+/-` around $3$ floors) and the huge standard error values. If we visualize the results (filtering out `r sum(arog_data$selected_data$numberOfFloors > arog_report$MAX_NUM_FLOORS_TO_CONSIDER, na.rm = TRUE)` flats with more than `r arog_report$MAX_NUM_FLOORS_TO_CONSIDER` floors), we see that:

```{r, echo = F}
arog_data$selected_data %>%
  filter(!is.na(numberOfFloors) & !is.na(typeOfFlat) &
           (numberOfFloors <= arog_report$MAX_NUM_FLOORS_TO_CONSIDER)) %>%
  group_by(typeOfFlat) %>%
  ggplot() +
    geom_bar(aes(numberOfFloors)) +
    scale_y_log10() +
    facet_wrap(~typeOfFlat, nrow=3)
```
The data seems to be normally distributed (except for the `apartment` type) with the mean values within $2.5$ - $4.0$ range. This makes us believe that this data is too much biased and polluted. So we will not rely on this column in our analysis.

#### Column: `condition` - $25.4$% `N/A` values
The condition factor levels are: 
```{r, echo = F}
levels(arog_data$selected_data$condition)
```
So we shall introduce a new level for the `N/A` and `""` values, called `"unknown"`.

#### Column: `yearConstructed` - $21.3$% `N/A` values
There is no good default to replace the `N/A` values here. Yet, it is a significant amount of data which we do not want to exclude. Therefore drop this column from the analysis and just use the `newlyConst` flag column.

#### Column: `floor` - $19.0$% `N/A` values
We could assign some `floor` values based on the flat types:
```{r, echo = F}
levels(arog_data$selected_data$typeOfFlat)
```
For example, we could consider assigning:

* `half_basement` -- the average `floor` for the half-basement
* `ground_floor` -- `floor = 0`
* `raised_ground_floor` -- the average `floor` for the raised ground floor

but, let us look at the floor values (filtering out `r sum(arog_data$selected_data$floor > arog_report$MAX_FLOOR_TO_CONSIDER, na.rm = TRUE)` flats higher that at the `r arog_report$MAX_FLOOR_TO_CONSIDER`'th floor), for these flat types:
```{r, echo = F}
arog_data$selected_data %>%
  filter(!is.na(floor) & !is.na(typeOfFlat) & (floor <= arog_report$MAX_FLOOR_TO_CONSIDER) &
           (typeOfFlat %in% c("half_basement", "ground_floor", "raised_ground_floor"))) %>%
  group_by(typeOfFlat) %>%
  ggplot() +
    geom_bar(aes(floor)) +
    scale_y_log10() +
    facet_wrap(~typeOfFlat, nrow=3)
```
From the data above we see that we shall not only correct the `N/A` values but set all of the floor values for the considered flat types as follows:

* `half_basement` --`floor = -1`
* `ground_floor` -- `floor = 0`
* `raised_ground_floor` --`floor = 0`

```{r, echo = F}
num_rem_na_floors <- arog_data$selected_data %>%
  filter(is.na(floor) & !is.na(typeOfFlat) &
           !(typeOfFlat %in% c("half_basement", "ground_floor", "raised_ground_floor"))) %>%
  nrow()
```

If we do that then there will still be `r num_rem_na_floors` (`r round(num_rem_na_floors*100/nrow(arog_data$selected_data), 1)`% of data) `N/A` floor values for the flat types for which we can not give any exact value. So we will just assign those to the mean floor value in the category.

#### Column: `heatingType` - $16.4$% `N/A` values
The heating type factor levels are: 
```{r, echo = F}
levels(arog_data$selected_data$heatingType)
```
So we shall introduce a new level for the `N/A`, `""`, and `"H"` values, called `"unknown"`.

#### Column: `typeOfFlat` - $13.9$% `N/A` values
The type of flat factor levels are: 
```{r, echo = F}
levels(arog_data$selected_data$typeOfFlat)
```
So we shall introduce a new level for the `N/A` and `""` values, called `"unknown"`. Note that, we do not use the pre-defined level `"other"` here as we interpret it as known flat type which is just not on the list of available choices.

#### Column `serviceCharge` - $2.58$% `N/A` values
We will set the service charges for the `N/A` values to zero. The motivation is that, there are:
```{r}
x <- arog_data$selected_data %>% filter(!is.na(serviceCharge))
sum(x$serviceCharge == 0)
```
zero values present, so we interpret the `N/A` values as defining the fact of no additional service charges.

### Additional steps
In addition to the data alternations done above we have also done the following:

* Re-setting the number of floors:
    * `half_basement` --`floor` $= -1$
    * `ground_floor` -- `floor` $= 0$
    * `raised_ground_floor` --`floor` $= 0$
* Filter out flats:
    * With `floor` $>$ `r arog_report$MAX_FLOOR_TO_CONSIDER`
    * Other than `"half_basement"`, `"other"`, and `"unknown"`; but with `floor` < $0$ 

## Restructuring data

There data set at hand does not have any complex structure. However, because we want to be able to do predictions per city and avoid cities with the same names within different lands and regions we shall combine the `regio` columns into a new single one, as follows:
```{r, eval =F}
  clean_arog_data <- clean_arog_data %>% 
    unite("location", c("regio1", "regio2", "regio3"), remove=FALSE) %>%
    select(-regio1, -regio2, -regio3)
```
The resulting columns have values constructed according to the following pattern:
```
location = regio1 + "_" + regio2 + "_" + regio3
```
For example:
```{r}
arog_data$wrangled_data$location[1:5]
```

## Wrangled data set
Let us now summarize the resulting clean data:

```{r, echo=F}
clean_data_na_cnts <- count_na_entries(arog_data$wrangled_data)
clean_data_na_cnts%>% print(., n=Inf)
```

```{r, echo = F}
selected_data_row_cnt <- nrow(arog_data$selected_data)
cleaned_data_row_cnt <- nrow(arog_data$wrangled_data)
na_total_rent_cnt <- round(selected_data_row_cnt/100*total_rent_na_percent)
selected_wtr <- nrow(arog_data$selected_data) - na_total_rent_cnt
```

As one can notice, the dat set size has been reduced from `r  nrow(arog_data$selected_data)` to `r cleaned_data_row_cnt` . The major reason for that is excluding the rows with the `N/A` values of the `totalRent` column. Let us recall that the number of such raws was `r total_rent_na_percent`% of the data set, e.g. `r na_total_rent_cnt` rows. It now remains to notice that `r nrow(arog_data$selected_data)` $-$ `r na_total_rent_cnt` $=$ `r selected_wtr` $\approx$ `r cleaned_data_row_cnt`. The remaining `r round( (selected_wtr - cleaned_data_row_cnt)*100/selected_data_row_cnt, 1)`% delta is explained by cleaning the `floor`/`typeOfFlat` columns and etc.

The data has been cleaned but we can expect that there is some noise in the data which we have not addressed. We might get more data-quality insights when we perform data analysis in the subsequent sections.

## Splitting data

To facilitate supervised learning, the wrangled data is split into the `modeling`, `r (1-arog_report$VALIDATION_TO_MODELING_SET_RATIO)*100`% of data, and `validation`, `r arog_report$VALIDATION_TO_MODELING_SET_RATIO*100`% of data, sets. The former will be used for training statistical model(s) and the latter for the model(s) validation. Note that, for the sake of subsequent cross validation during modeling part, we further split the `modeling` set into the `training`, `r (1-arog_report$TEST_TO_TRAIN_SET_RATIO)*100`% thereof, and `testing`, `r arog_report$TEST_TO_TRAIN_SET_RATIO*100`% thereof, sets.

We split the data in the following steps:

1. The data is randomly split into to parts according to the specified ratio:
```{r, eval = F}
  test_index <- createDataPartition(y = data_set$totalRent, times = 1, 
                                    p = ratio, list = FALSE)
```
2. The `test_index` rows are the candidates for the `testing`/`validation` set rows
3. The factorized column values of the `testing`/`validation` set are considered:
```{r}
str_data <- capture.output(str(arog_data$wrangled_data))
str_replace_all(str_subset(str_data, "Factor"), "\",..:.*","\",..")
```
4. The rows with the values not present in the `testing`/`modeling` set are dropped 
5. The `testing`/`modeling` set consists of rows absent in the `testing`/`validation` set

The procedure above ensures that the `testing`/`validation` set can always be evaluated on a model trained on the `testing`/`modeling` set. For more details, see the `create_arog_data` and `split_train_test_sets` functions located in the `apartment_rental_project.R` script.

The resulting set sizes are as follows:
```{r, echo = F}
total_data_size <- nrow(arog_data$wrangled_data)
train_set_size <- nrow(arog_data$training_data)
test_set_size <- nrow(arog_data$testing_data)
model_set_size <- train_set_size + test_set_size
valid_set_size <- nrow(arog_data$validation)
```
* `modeling` -- `r model_set_size` rows, `r round(model_set_size*100/total_data_size, 1)`% of data
    * `training` -- `r train_set_size` rows, `r round(train_set_size*100/model_set_size, 1)`% of `modeling` set
    * `testing` -- `r test_set_size` rows, `r round(test_set_size*100/model_set_size, 1)`% of `modeling` set
* `validation` -- `r valid_set_size` rows, `r round(valid_set_size*100/total_data_size, 1)`% of data

As expected, due to returning `testing`/`validation` set rows to the `testing`/`modeling` set for consistency, the desired set ratios are biased. The `validation` set size is almost as prescribed (`r arog_report$VALIDATION_TO_MODELING_SET_RATIO*100`% of data), but the `testing` set size is affected more significantly[^6]. Yet, we see no issue as the `testing` set is still $> 10$% of the `modeling` set, which should be enough for performing cross validation.

[^6]: The requested `testing` set size was `r arog_report$TEST_TO_TRAIN_SET_RATIO*100`% of the `modeling` set.

# Data analysis
<!-- A data analysis section that:
 1. Explains the process and techniques used, such as:
  1.1 Data exploration
  1.2 Data visualization
 2. Presents insights gained -->

# Modeling approach
<!-- A modeling approach section that explains the modeling approach -->



# Results
<!-- A results section that:
 1. Presents the modeling results 
 2. Discusses the model performance -->



# Conclusions
<!-- A conclusion section that:
 1. Gives a brief summary of the report
 2. Explains its limitations
 3. Suggests future work -->
 
## Future work

Check for introducing any bias by data wrangling.

# Appendix A: The complete list of data set columns

Hereby we present the list of columns from the original data set:

```{r, echo = F}
names(arog_data$raw_data)
```

# Appendix B: Data set column descriptions

Here is the list of the initially considered data set columns with the descriptions thereof:

1. `hasKitchen` -- has a kitchen
2. `balcony` -- does the object have a balcony
3. `cellar` -- has a cellar
4. `lift` -- is elevator available
5. `floor` -- which floor is the flat on
6. `garden` -- has a garden
7. `noParkSpaces` -- number of parking spaces
8. `livingSpace` -- living space in sqm
9. `condition` -- condition of the flat
10. `interiorQual` -- interior quality

11. `regio1` --  Bundesland
12. `regio2` - District or Kreis, same as geo krs
13. `regio3` -- City/town

14. `noRooms` -- number of rooms
15. `numberOfFloors` -- number of floors in the building
16. `typeOfFlat` -- type of flat
17. `yearConstructed` -- construction year
18. `newlyConst` -- is the building newly constructed
19. `heatingType` -- Type of heating
20. `energyEfficiencyClass` -- energy efficiency class

21. `heatingCosts` -- monthly heating costs in €
22. `serviceCharge` --  auxiliary costs such as electricity or Internet in €
23. `electricityBasePrice` -- monthly base price for electricity in €
24. `baseRent` -- base rent without electricity and heating
25. `totalRent` -- total rent (usually a sum of base rent, service charge and heating cost)

26. `date` -- time of scraping
